{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \u003cimg style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"\u003e CS109B Introduction to Data Science\n",
                "\n",
                "## Lab 6: CNNs, Autoencoders, Tensorflow Datasets, and Transfer Learning\n",
                "\n",
                "**Harvard University**\u003cbr/\u003e\n",
                "**Spring 2025**\u003cbr/\u003e\n",
                "**Instructors**: Pavlos Protopapas, Natesh Pillai, and Chris Gumb\u003cbr/\u003e\n",
                "\u003cbr/\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES\n",
                "import requests\n",
                "from IPython.core.display import HTML\n",
                "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\").text\n",
                "HTML(styles)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003ca id=\"contents\"\u003e\u003c/a\u003e\n",
                "\n",
                "## Notebook Contents\n",
                "- [**Keras for CNNs**](#keras)\n",
                "    - [Layers of a CNN in Keras](#keras_layers)\n",
                "    - [**Exercise:** Building a CNN from Scratch](#build_cnn)\n",
                "- [**CNNs and Autoencoders with KMNIST**](#kmnist)\n",
                "    - [Baseline CNN Classifier](#baseline)\n",
                "    - [**Exercise:** Improving on Baseline Model](#improving)\n",
                "- [**Tensorflow Datasets**](#tfdatasets)\n",
                "    - [Loading Datasets](#loadds)\n",
                "    - [The Dataset Object](#dsobj)\n",
                "    - [Take, Cardinality, \u0026 Batch](#take)\n",
                "    - [Cache, Prefetch, \u0026 Shuffle](#cache)\n",
                "    - [Preprocessing with Datasets](#dspreproc)\n",
                "    - [Data Augmentation](#dataaug)\n",
                "- [**Transfer Learning**](#transfer_learning)\n",
                "    - [MobileNet](#mobilenet)\n",
                "\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if GPU is available and visible to TensorFlow\n",
                "import tensorflow as tf\n",
                "if tf.test.gpu_device_name():\n",
                "    print('GPU found')\n",
                "else:\n",
                "    print(\"No GPU found\")"
            ]
        },
        {
            "attachments": {
                "ec1d1aa0-3e29-4734-9171-fa26d58ed5cb.png": {
                    "image/png": "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"
                }
            },
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003ca id='keras'\u003e\u003c/a\u003e\n",
                "## Keras [^](#contents \"Back to Contents\")\n",
                "![image.png](attachment:ec1d1aa0-3e29-4734-9171-fa26d58ed5cb.png)\n",
                "\n",
                "\n",
                "The \u003ca href='https://keras.io/'\u003eKeras API\u003c/a\u003e sits on top of Tensorflow. It allows users to work at a more intuitive level of abstraction where the basic objects are **layers**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import keras"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ensure replicable results\n",
                "import os\n",
                "import random as rn\n",
                "SEED = 109\n",
                "tf.random.set_seed(SEED)\n",
                "os.environ['PYTHONHASHSEED'] = '0'\n",
                "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
                "tf.random.set_seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "rn.seed(SEED)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from keras.layers import Activation, BatchNormalization, Conv2D, Dense, Dropout, Flatten, GaussianNoise\n",
                "from keras.layers import GlobalAveragePooling2D, Input, MaxPool2D, MaxPooling2D, RandomRotation, UpSampling2D\n",
                "from keras.models import Model, Sequential\n",
                "from keras import losses\n",
                "from keras import optimizers\n",
                "from keras import layers\n",
                "from keras import callbacks"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=\"keras_layers\" class='exercise'\u003e\u003cb\u003eLayers of a CNN in Keras\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\u003cimg src='fig/cnn1.png' width='900px'\u003e\n",
                "\n",
                "The following is a list of layers commonly used when building CNNs with Keras.\u003cbr\u003e\n",
                "A link to the official documentation for each layer is also provided."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=\"build_cnn\" class='exercise'\u003e\u003cb\u003eBuild a CNN from Scratch\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "Now let's try building a CNN model with a similar architecture to [**LeNet**](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf) from scratch, using the layers below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build our LeNet model from scratch with Keras Function API\n",
                "\n",
                "# The expected input size is 32x32 grayscale images\n",
                "# your code here\n",
                "\n",
                "# Convolution Layer 1: 6 filters of size 5x5, stride=1, no padding\n",
                "# your code here\n",
                "\n",
                "# Pooling Layer 1: Max Pooling with pool size 2x2, stride 2\n",
                "# your code here\n",
                "\n",
                "# Convolution Layer 2: 16 filters of size 5x5, stride=1, no padding\n",
                "# your code here\n",
                "\n",
                "# Pooling Layer 2: Max Pooling with pool size 2x2, stride 2\n",
                "# your code here\n",
                "\n",
                "# Convolution Layer 3: 120 filters of size 5x5\n",
                "# your code here\n",
                "\n",
                "# Flatten before fully-connected layers\n",
                "# your code here\n",
                "\n",
                "# Fully Connected Layer: 84 units with relu activation\n",
                "# your code here\n",
                "\n",
                "# Output Layer: [10-class Image Classification Task]\n",
                "# your code here\n",
                "\n",
                "# Instantiate your model\n",
                "# your code here\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Input\n",
                "\n",
                "[**tf.keras.Input**](https://www.tensorflow.org/api_docs/python/tf/keras/Input)(\n",
                "    shape=None, **kwargs\n",
                ")\n",
                "\n",
                "\u003cdiv style='color:red'\u003e\u003cstrong\u003eThe input is not a layer!\u003c/strong\u003e\u003c/div\u003e\n",
                "\n",
                "As Pavlos said in lecture, you shouldn't think of the input to your network as a layer. Unfortunately, in Keras, most components of a network are referred to as 'layers'. Someone must have come to their senses because now `Input` can be found in the base `tf.keras` module. While it *can* still be imported from `tf.keras.layers`, we are civilized people and shall speak no more of that.\n",
                "\n",
                "The network will be expecting input of fixed shape which must be specified with the `shape` parameter. You should look at the data you are using to determine this shape.\n",
                "\n",
                "Adding an explicit `Input` object to your layer is not required as most layers have an `input_shape` that can be specified if they are the first layer in the network.\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2D Convolutional Layers\n",
                "\n",
                "[**keras.layers.Conv2D**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) (filters, kernel_size, strides=(1, 1), padding='valid', activation=None, use_bias=True, \n",
                "                    kernel_initializer='glorot_uniform', data_format='channels_last', \n",
                "                    bias_initializer='zeros')\n",
                "\n",
                "\u003cimg src='fig/conv-many-filters.png' width='550px'\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Some quick review if skipping to this section:**\n",
                "\n",
                "A convolutional layer is composed of **filters**, which are composed of **kernels** which are themselves composed of **weights**. Each filter also has a bias term though it is often not depicted in diagrams (it is exluded in the one above for example). We learn the weights and biases from our data. Each conv layer also has an associated **activation function** such as ReLU or sigmoid. \n",
                "\n",
                "The **number of filters** and the **height and width of the kernels** of which they consist are set by the `filters` and `kernel_size` (a tuple) arguments respectively. \n",
                "\n",
                "The **depth of the filters is fixed** by the depth (i.e., 'channels' or 'filter maps') of the input to the conv layer. \n",
                "\n",
                "The output of the conv layer is is a 3D tensor which is a set of **feature maps**. Each feature map is itself the output of one of the layer's filters convolving on the input. The height and width of the feature map tensor is determined by the input size, `kernel_size`, `padding`, and `stride`. The depth of the output tensor (i.e, number of feature maps) is equal to the number of filters in the layer.\n",
                "                    \n",
                "Keras also has a [1D convolutional layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) used for time series data and a [3D convolutional layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv3D) used for video."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pooling Layers\n",
                "\n",
                "[**keras.layers.MaxPool2D**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n",
                "\n",
                "\u003cimg src='fig/maxpool.png' alt='MaxPool' width='400px'\u003e\n",
                "\n",
                "Pooling layers are also comprised of filters and feature maps. Let's say the pooling layer has a 2x2 receptive field and a stride of 2. This stride results in feature maps that are one half the size of the input feature maps. We can use a max() operation for each receptive field. "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Flatten Layers\n",
                "\n",
                "\n",
                "[**keras.layers.Flatten**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten)()\n",
                "\n",
                "Like `Input` and `Dropout`, `Flatten` is not a layer in the traditional sense. It has no learned parameters and no parameters other than `input_shape`. Its only function is to flatten its multi-dimensional input into a flat vector. The flatten layer sits between our final 2D output (either from Conv2D or MaxPool2D) and our first fully connected, `Dense` layer.\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Fully Connected Layers\n",
                "\n",
                "[**keras.layers.Dense**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)(units, activation=None, use_bias=True, \n",
                "                    kernel_initializer='glorot_uniform', bias_initializer='zeros')\n",
                "                    \n",
                "\u003cimg src='fig/dense.png' width='250px'\u003e\n",
                "\n",
                "Most CNNs have of one or more dense layers at the end with the final layer referred to as the **output layer**. You'll need to specify the number of `units` in each layer (sometimes called 'neurons' or 'nodes') as well as the `activation`. \n",
                "\n",
                "*Special care should be taken in deciding on the activation function for the output layer!* The correct choice of activation in this final layer depends on the task we are training our model to perform. For example, a linear activation for regression, but a sigmoid for binary classification.\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Dropout Layers\n",
                "\u003cimg src='fig/dropout.gif' width='200px'\u003e\n",
                "\n",
                "[**tf.keras.layers.Dropout**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)(rate, seed=None)\n",
                "\n",
                "Dropout consists in randomly setting a fraction of input units to 0 at each update during training time. In Keras this fraction is set by the `rate` parameter. At inference time, trained weights are multipled by $(1 - \\text{rate})$. Dropout often used to help prevent overfitting by limiting the complexity of our model. It can also prevent groups of neurons from 'conspiring' together to have a large affect on the out put, something traditional forms of weight regularization would not catch.\n",
                "\n",
                "**Caution:** Dropout's behavior is not the same if performed after a convolutional layer!\n",
                "\n",
                "**Q:** Why might it make sense to think of dropout as a type of ensemble method? ü§î\n",
                "\n",
                "References\u003cbr\u003e\n",
                "[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Global Average Pooling Layers\n",
                "\n",
                "[**keras.layers.GlobalAveragePooling2D**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D)()\n",
                "\n",
                "The GlobalAveragePooling2D layer is a type of pooling layer that is commonly used in convolutional neural networks (CNNs), particularly towards the end of the network, before the final classification layers as an alternative to `Flatten`.\n",
                "\n",
                "**What is Global Average Pooling (GAP)?**\n",
                "\n",
                "Global Average Pooling (GAP) is an operation that calculates the average value of each feature map in the previous layer. Unlike traditional pooling layers that operate on local regions (e.g., 2x2 pools), GAP processes the entire feature map. For a given feature map, GAP outputs a single average value, effectively reducing the spatial dimensions (width and height) of the feature map to 1. If a convolutional layer outputs H x W x C feature maps (H being height, W width, and C channels or depth), applying GAP will reduce this to 1 x 1 x C, transforming spatial features into a flat vector.\n",
                "\n",
                "**Why Use Global Average Pooling?**\n",
                "\n",
                "- **Reduces Overfitting:** By summarizing the spatial information, GAP reduces the total number of parameters in the model. This simplification can help prevent overfitting, as there are fewer parameters to learn compared to when using fully connected layers directly after convolutional layers.\n",
                "\n",
                "- **Seamless Transition to Classification:** By outputting a flat vector that corresponds to the number of feature maps, GAP provides an efficient way to transition from convolutional layers (which are good at extracting spatial features) to dense layers (used for classification). The output of GAP can be directly fed into a dense layer without the need for reshaping or flattening."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003ca id='kmnist'\u003e\u003c/a\u003e\n",
                "## CNNs and Autoencoders with KMNIST [^](#contents \"Back to Contents\")\n",
                "\n",
                "\n",
                "## The Kannada MNIST Dataset\n",
                "\n",
                "[Return to contents](#contents)\n",
                "\n",
                "\n",
                "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3630446%2F1e01bcc28b5ccb7ad38a4ffefb13cde0%2Fwondu.png?generation=1603204077179447\u0026alt=media)\n",
                "\n",
                "\n",
                "For this part, we will be working with a modified version of the [Kannada MNIST dataset](https://arxiv.org/pdf/1908.01242.pdf), which is a large database of handwritten digits in the indigenous language *Kannada*.\n",
                "\n",
                "This dataset consists of 60,000 28x28 grayscale images of the ten digits, along with a test set of 10,000 images. For this lab, we will simplify the problem by only using the digits labeled `0` and `1` owing to the similarity of the two symbols, and we will use a total of 1,200 samples for training (including the data for validation), and 2,000 samples for test.\n",
                "\n",
                "To understand the dataset better, we recommend this [article](https://towardsdatascience.com/a-new-handwritten-digits-dataset-in-ml-town-kannada-mnist-69df0f2d1456) by Vinay Prabhu, the curator of the dataset.\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load X and y datasets from .csv files\n",
                "X_kmnist_train = pd.read_csv(\"kmnist_data/kmnist_train.csv\").drop(columns=\"output\").values\n",
                "y_kmnist_train = pd.read_csv(\"kmnist_data/kmnist_train.csv\")[\"output\"].values\n",
                "\n",
                "X_kmnist_test = pd.read_csv(\"kmnist_data/kmnist_test.csv\").drop(columns=\"output\").values\n",
                "y_kmnist_test = pd.read_csv(\"kmnist_data/kmnist_test.csv\")[\"output\"].values\n",
                "\n",
                "# reshape both X frames for proper pixel representation in imshow plots\n",
                "X_kmnist_train = X_kmnist_train.reshape(-1, 28, 28, 1)/255.\n",
                "X_kmnist_test = X_kmnist_test.reshape(-1, 28, 28, 1)/255.\n",
                "\n",
                "print(\n",
                "    \"The shapes of the Kannada MNIST X and y datasets are:\\n\\n\"\n",
                "    \"\\tX train\\t{}\\n\\ty train\\t{}\\n\\n\\tX test\\t{}\\n\\ty test\\t{}\".format(\n",
                "        X_kmnist_train.shape, y_kmnist_train.shape, X_kmnist_test.shape, y_kmnist_test.shape\n",
                "    )\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's visualize what the image data of handwritten 0 and 1 look like."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize Image Data\n",
                "\n",
                "idx_list = [2, 1]\n",
                "fig, axes = plt.subplots(1, 2, figsize=(6, 3.5))\n",
                "\n",
                "plt.suptitle(\n",
                "    \"Sample KMNIST training characters\",\n",
                "    y=1,\n",
                "    fontsize=18,\n",
                ")\n",
                "\n",
                "for idx, ax in zip(idx_list, axes.flat):\n",
                "    ax.imshow(X_kmnist_train[idx], cmap=\"gray\")\n",
                "    ax.set_xticks([])\n",
                "    ax.set_yticks([])\n",
                "    ax.set_title(\n",
                "        \"class label: {}\".format(y_kmnist_train[idx]),\n",
                "        fontsize=14,\n",
                "    )\n",
                "\n",
                "plt.tight_layout()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now let's build a fully-connected network in a similar way to what we've done in HW2. We will use only dense layers and apply some of the regularization methods."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper functions to print training history and test results\n",
                "\n",
                "def plot_hist(model, title=None):\n",
                "    fig, axs = plt.subplots(1,2, figsize=(12,5))\n",
                "    axs[0].plot(model.history.history['loss'], label='train')\n",
                "    axs[0].plot(model.history.history['val_loss'], label='val')\n",
                "    axs[0].axvline(np.argmin(model.history.history['val_loss']), c='k', ls=':', label='final model')\n",
                "    axs[0].set_xlabel('Epoch')\n",
                "    axs[0].set_ylabel('BCE Loss')\n",
                "    axs[0].legend()\n",
                "    axs[1].plot(model.history.history['accuracy'], label='train')\n",
                "    axs[1].plot(model.history.history['val_accuracy'], label='val')\n",
                "    axs[1].axvline(np.argmin(model.history.history['val_loss']), c='k', ls=':', label='final model')\n",
                "    axs[1].legend()\n",
                "    axs[1].set_xlabel('Epoch')\n",
                "    axs[1].set_ylabel('ACC')\n",
                "    plt.suptitle(title)\n",
                "\n",
                "\n",
                "def print_test_results(model_name, test_results):\n",
                "    print(f\"{model_name} performance on test set:\\n\\t\"\n",
                "          +f\"Test Loss: {test_results[0]:.4f}\\n\\tTest Accuracy: {test_results[1]:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fit a fully-connected model with some regularization\n",
                "\n",
                "# regularization parameters\n",
                "dropout = 0.1\n",
                "kernel_weight = 0.003\n",
                "bias_weight = 0.003\n",
                "\n",
                "# create fully-connected network with regularization and dropout\n",
                "fcn_model = tf.keras.models.Sequential(\n",
                "    [\n",
                "        tf.keras.layers.InputLayer(input_shape=(784,)),\n",
                "        tf.keras.layers.Dense(\n",
                "            100, activation=\"relu\",\n",
                "            kernel_regularizer=tf.keras.regularizers.l2(kernel_weight),\n",
                "            bias_regularizer=tf.keras.regularizers.l2(bias_weight),\n",
                "        ),\n",
                "        tf.keras.layers.Dropout(dropout),\n",
                "        tf.keras.layers.Dense(\n",
                "            100, activation=\"relu\",\n",
                "            kernel_regularizer=tf.keras.regularizers.l2(kernel_weight),\n",
                "            bias_regularizer=tf.keras.regularizers.l2(bias_weight),\n",
                "        ),\n",
                "        tf.keras.layers.Dropout(dropout),\n",
                "        tf.keras.layers.Dense(\n",
                "            100, activation=\"relu\",\n",
                "            kernel_regularizer=tf.keras.regularizers.l2(kernel_weight),\n",
                "            bias_regularizer=tf.keras.regularizers.l2(bias_weight),\n",
                "        ),\n",
                "        tf.keras.layers.Dropout(dropout),\n",
                "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
                "    ]\n",
                ")\n",
                "\n",
                "fcn_model.summary()\n",
                "\n",
                "early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
                "\n",
                "# compile model\n",
                "fcn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
                "\n",
                "# fit model using the augmented training set\n",
                "fcn_model.fit(X_kmnist_train.reshape(-1, 784), \n",
                "              y_kmnist_train,\n",
                "              batch_size=64,\n",
                "              validation_split=0.2,\n",
                "              epochs=100,\n",
                "              verbose=0,\n",
                "              callbacks=[early_stop]\n",
                ")\n",
                "\n",
                "plot_hist(fcn_model, 'FCN Classifier')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "print_test_results(\"Fully-connected network\",\n",
                "                   fcn_model.evaluate(X_kmnist_test.reshape(-1, 784), y_kmnist_test))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=\"baseline\" class='exercise'\u003e\u003cb\u003eBaseline CNN Classifer\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = Input(shape=X_kmnist_train.shape[1:])\n",
                "x = Conv2D(64, 2, activation='relu', padding='same')(inputs)\n",
                "x = MaxPool2D(2)(x)\n",
                "x = Conv2D(32, 2, activation='relu', padding='same')(x)\n",
                "x = MaxPool2D(2)(x)\n",
                "x = Conv2D(16, 2, activation='relu', padding='same')(x)\n",
                "x = MaxPool2D(2)(x)\n",
                "x = Flatten()(x)\n",
                "x = Dense(64, activation='relu')(x)\n",
                "outputs = Dense(1, activation='sigmoid')(x)\n",
                "\n",
                "baseline_cnn = Model(inputs=inputs, outputs=outputs)\n",
                "baseline_cnn.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "baseline_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
                "baseline_cnn.fit(X_kmnist_train,\n",
                "             y_kmnist_train,\n",
                "             validation_split=.2,\n",
                "             batch_size=64,\n",
                "             callbacks=early_stop,\n",
                "             verbose=0,\n",
                "             epochs=50)\n",
                "\n",
                "plot_hist(baseline_cnn, 'Baseline CNN Classifier')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "print_test_results(\"Baseline CNN model\",\n",
                "                   baseline_cnn.evaluate(X_kmnist_test, y_kmnist_test))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=\"improving\" class='exercise'\u003e\u003cb\u003eImproving on the Baseline\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# your code here\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003ca id='tfdatasets'\u003e\u003c/a\u003e\n",
                "\n",
                "## Tensorflow Datasets [^](#contents \"Back to Contents\")\n",
                "\u003cimg src='https://3.bp.blogspot.com/-d-nV7xJRmpw/Xo328dcAx3I/AAAAAAAAC7Q/qlqJOle6XIosJ3CGIDJ04F3Voh1iXDg0gCLcBGAsYHQ/s1600/TF_FullColor_Icon.jpg' width='150'\u003e\n",
                "\n",
                "TensorFlow Datasets (TFDS) is a collection of datasets ready to use, with TensorFlow or other Python ML frameworks. These datasets are exposed as `tf.data.Dataset` objects, enabling easy-to-use and high-performance input pipelines."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import tensorflow_datasets as tfds"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Better performance with the tf.data API\n",
                "# Reference: https://www.tensorflow.org/guide/datac_performance\n",
                "AUTOTUNE = tf.data.experimental.AUTOTUNE"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=\"loadds\" class='exercise'\u003e\u003cb\u003eLoading Datasets\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "TFDS gives us access to dozens of research quality datasets with the simple `tfds.load` method.\u003cbr\u003e\n",
                "An extensive catalogue of datasets can be seen \u003ca href='https://www.tensorflow.org/datasets/catalog/overview'\u003ehere\u003c/a\u003e. You can even \u003ca href='https://www.tensorflow.org/datasets/add_dataset'\u003ewrite your own custom dataset\u003c/a\u003e.\n",
                "\n",
                "### \u003cdiv style='font-size: 150%'\u003eüêé or üßç?\u003c/div\u003e\n",
                "\n",
                "But it doesn't mean you should just because you can. The *bizzare* \u003ca href='http://laurencemoroney.com/horses-or-humans-dataset'\u003eHorses or Humans\u003c/a\u003e dataset may just be an example of this.\u003cbr\u003e\n",
                "Our call to `tfds.load` will use several arguments:\n",
                "- `name`: (str) the dataset to load (you can look these up in the above catalogue)\n",
                "- `split`: (list) some datasets have pre-specified splits; this list defines which splits to load\n",
                "- `shuffle_files`: (bool) files are loaded in random order\n",
                "- `as_supervised`: (bool) loads labels if dataset has them\n",
                "- `with_info`: (bool) also returns an DatasetInfo object with details about the loaded dataset\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "tfds.disable_progress_bar()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "(ds_train, ds_test), ds_info = tfds.load(name=\"horses_or_humans\", split=['train', 'test'],\n",
                "                                         shuffle_files=True, as_supervised=True, with_info=True, )"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The `ds_info` we got from using `with_info=True` gives us a great overview of some facts about the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "ds_info"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=\"dsobj\" class='exercise'\u003e\u003cb\u003eThe Dataset Object\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "We'll be working with closely with the `tf.data.Dataset` object so we should learn more about its methods and structure."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Some Python Arcana: Iterables \u0026 Iterators**:\u003cbr\u003e\n",
                "\n",
                "The `tf.data.Dataset` object is an *iterable*, which means it implements an `__iter__` method which returns an *iterator* object.\u003cbr\u003e\n",
                "An *iterator* is an object that implements a `__next__` method which returns the next element in the iterator!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create iterator from iterable\n",
                "my_iter = iter(ds_train)\n",
                "my_iter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get next element from iterator\n",
                "next_one = next(my_iter)\n",
                "print(f'Each element in the iterator is of type {type(next_one)} with length {len(next_one)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# display element from iterator\n",
                "next_one"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Inspecting this tuple we see the 1st element is our image and the 2nd is the lable. Let's visualize the image."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "image, label = next_one\n",
                "plt.imshow(image)\n",
                "plt.title(int(label));"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It appears humans are the positive class. Let's make a dictionary to map class labels to strings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create human interperatable class names\n",
                "class_names = {0: 'horse', 1: 'human'}"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Iterating**\n",
                "\n",
                "Iterables can be looped over with a `for` loop. And while you *can* loop over the `Dataset` iterable object itself it is more common to first use the `as_numpy_iterator()` method if running TF in eager mode."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "rows = 3\n",
                "cols = 5\n",
                "fig, ax = plt.subplots(rows, cols, figsize=(10,6))\n",
                "for ax, (img, label) in zip(ax.ravel(), ds_train.as_numpy_iterator()):\n",
                "    # break when no more axes left\n",
                "    if ax is None:\n",
                "        break\n",
                "    ax.imshow(img)\n",
                "    ax.set_title(class_names[label])\n",
                "    ax.axis('off')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The above method works, but it would be a nuisance to have to always write out all that code just to inspect our datasets.\n",
                "\n",
                "Luckily, `tfds` has a much faster way to do this with the `show_examples()` method."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# train examples\n",
                "tfds.show_examples(ds_train, ds_info, rows=rows, cols=cols)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# test examples\n",
                "tfds.show_examples(ds_test, ds_info, rows=rows, cols=cols)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q:** Do you notice anything strange about the test examples? ü§î"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=\"take\" class='exercise'\u003e\u003cb\u003eTake, Cardinality, \u0026 Batch\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "**Take**\n",
                "\n",
                "We can use the `take()` method to return a **subset** of the original `Dataset` object of a desired **cardinality**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get a subset with cardinality 2\n",
                "my_subset = ds_train.take(2)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Cardinality**\n",
                "\n",
                "It's true that we can check the length of a datset with `len()`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# one way to find a Dataset's length\n",
                "len(my_subset)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "But this is inefficient  for larger datasets.\u003cbr\u003e\n",
                "It is preferable to use the `cardinality()` method. This returns a Tensor."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# a better way\n",
                "cardinality = my_subset.cardinality()\n",
                "print(f'Cardinality Type: {type(cardinality)}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It looks a bit strange when displayed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# displaying an EagerTensor\n",
                "cardinality"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "But it behaves just like an integer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# no surprises here!\n",
                "(cardinality + 2) == 4"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And if you really want to, you can always convert it to a `numpy.int64` object which prints nicely."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# convert to nump.int64\n",
                "cardinality.numpy()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Batch**\n",
                "\n",
                "To get the benefits of \u003ca href='https://en.wikipedia.org/wiki/Stochastic_gradient_descent'\u003e**stochastic gradient descent**\u003c/a\u003e (SGD) during training, we'd like to feed elements from the dataset into our model in batches.\u003cbr\u003e\n",
                "This is handled by the `batch()` method.\n",
                "\n",
                "**Q:** What are some benefits of SGD? ü§î"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 32\n",
                "num_batch = ds_train.cardinality() / BATCH_SIZE\n",
                "print(f'Number of Potential Batches of size {BATCH_SIZE}:', num_batch.numpy())\n",
                "num_batched_produced = ds_train.batch(BATCH_SIZE).cardinality()\n",
                "print(f'Number of Batches of size {BATCH_SIZE} Produced:', num_batched_produced.numpy())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q:** Why don't these numbers match? What's going on? ü§î"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The batched dataset is itself a `Dataset`, but now it's an iterable that produces batches.\u003cbr\u003e\n",
                "In a **supervised** situation like ours, each batch is a tuple."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect first batch\n",
                "my_batch = ds_train.batch(BATCH_SIZE).as_numpy_iterator().next()\n",
                "print(f'Each batch is of type {type(my_batch)} with length {len(my_batch)}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The 1st element of the tuple are all the images in the batch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# batch images\n",
                "my_batch[0].shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The 2nd element are all the labels in the batch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# batch labels\n",
                "my_batch[1].shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Note:** The batch of images and labels above are both `numpy` arrays. This is because we used `as_numpy_iterator()` on our batched dataset. This is why we were able to use the `shape` attribute. here."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also strengthen our intuition about the structure of the batched dataset by iterating over the batches and displaying the first image in each."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# display the first image in each batch\n",
                "fig, axs = plt.subplots(4, 8, figsize=(9,5))\n",
                "axs = axs.ravel()\n",
                "for i, (img_batch, label_batch) in enumerate(ds_train.batch(BATCH_SIZE, drop_remainder=True)):\n",
                "    for (img, label) in zip(img_batch, label_batch):\n",
                "        axs[i].imshow(img)\n",
                "        axs[i].set_title(f'batch {i+1}')\n",
                "        axs[i].axis('off')\n",
                "        break\n",
                "plt.tight_layout()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=\"cache\" class='exercise'\u003e\u003cb\u003eCache, Prefetch \u0026 Shuffle\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "There are helpful methods we can use to optimize the training process. Most of these descriptions are adapted from the TensorFlow documentation. As always, the documentation is the best place to go if you'd like a deeper understanding.\n",
                "\n",
                "**\u003ca href=\"https://www.tensorflow.org/guide/data_performance#caching\"\u003e`Cache`\u003c/a\u003e** caches a dataset, either in memory or on local storage. This will save some operations (like file opening and data reading) from being executed during each epoch. (perhaps not a good idea for enormous datasets)\n",
                "\n",
                "**`Prefetching`** overlaps the preprocessing and model execution of a training step. While the model is executing training steps, the input pipeline is reading the data for step s+1. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data.\"\n",
                "\n",
                "**\u003ca href=\"https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\"\u003e`Shuffle`\u003c/a\u003e** Randomly shuffles the elements of this dataset.\n",
                "\n",
                "**Note:** cache will produce exactly the same elements during each iteration through the dataset. If you wish to randomize the iteration order, make sure to call shuffle after calling cache."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And of course we can chain all these commands together!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "images, labels = ds_train.cache()\\\n",
                "                         .shuffle(buffer_size=ds_train.cardinality(), seed=SEED, reshuffle_each_iteration=True)\\\n",
                "                         .batch(BATCH_SIZE).prefetch(AUTOTUNE)\\\n",
                "                         .as_numpy_iterator().next() # one batch\n",
                "# show first image in batch\n",
                "plt.imshow(images[0])\n",
                "plt.title(class_names[labels[0]])\n",
                "plt.axis('off')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=\"dspreproc\" class='exercise'\u003e\u003cb\u003ePreprocessing with Datasets\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "Often, we'll want to **preprocess** our data in some way before feeding it into our model.\u003cbr\u003e\n",
                "We can use use the `Dataset` object's `map` method to perform arbitrary functions on the elements of the dataset.\n",
                "\n",
                "Because the result of the `map` operation is itself a dataset object, we can continue to chain these commands one after another.\u003cbr\u003e\n",
                "Here we are normalizing and resizing our images as part of the preprocessing stage using functions of our own design. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "H = W = 224\n",
                "\n",
                "def normalize_img(img, label):\n",
                "    return tf.cast(img, tf.float32)/255.0, label\n",
                "\n",
                "def resize_img(img, label):\n",
                "    return tf.image.resize(img, size=[H, W]), label\n",
                "\n",
                "def preprocess(img, label):\n",
                "    img, label = normalize_img(img, label)\n",
                "    img, label = resize_img(img, label)\n",
                "    return img, label\n",
                "\n",
                "ds_train = ds_train.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
                "ds_test = ds_test.map(preprocess, num_parallel_calls=AUTOTUNE)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=\"dataaug\" class='exercise'\u003e\u003cb\u003eData Augmentation\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "\n",
                "We almost always wish we had *more data*! But it can be expensive and time consuming to gather and label new data.\u003cbr\u003e\n",
                "So why not **simulate new data?** We can accomplish this by creating variants of our original data. \n",
                "\n",
                "In the case of images this is very intuitive. Simply rotate your picture of a horse. It's still a horse, but the rotated image is likely different from anything in your data original. As long as the simulated data is not *too* different from the sort of example's we'd like to learn, this can help our model generalize better to previously unseen examples not in the original dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow_addons as tfa\n",
                "\n",
                "def random_rotate(image, label):\n",
                "    \"\"\"Dataset pipe that rotates an image, helper function to augment below\"\"\"\n",
                "    shape = image.shape\n",
                "    deg = tf.random.uniform([],-10.,10.)\n",
                "    image = tfa.image.rotate(image, deg/180.*np.pi, interpolation=\"BILINEAR\")\n",
                "    image.set_shape((shape))\n",
                "    label.set_shape(())\n",
                "    return image, label\n",
                "\n",
                "def random_zoom(image, label):\n",
                "    \"\"\"Dataset pipe that zooms an image, helper function to augment below\"\"\"\n",
                "    rand_float = tf.random.uniform([],10,20)\n",
                "    rand_int = tf.cast(rand_float, tf.int32)\n",
                "    image = tf.image.resize_with_crop_or_pad(image,\n",
                "                                             H + H//rand_int,\n",
                "                                             W + W//rand_int)\n",
                "    image = tf.image.random_crop(image, size=[H, W, 3])\n",
                "    return image, label\n",
                "    \n",
                "def augment(image, label):\n",
                "    \"\"\"Function that randomly alters an image with\n",
                "       flipping, rotation, zoom, and contrast adjustment\"\"\"\n",
                "    image = tf.image.random_flip_left_right(image)\n",
                "    image, label = random_rotate(image, label)\n",
                "    # image, label = random_zoom(image, label)\n",
                "    # image = tf.image.random_contrast(image, lower=.95, upper=1.)\n",
                "    \n",
                "    return image, label"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here are just a few examples created using the augmentation functions defined above!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# display a batch of altered images\n",
                "fig, axs = plt.subplots(4,4, figsize=(6,6))\n",
                "aug_batch = ds_train.map(augment, num_parallel_calls=AUTOTUNE).take(16)\n",
                "for ax, (img, label) in zip(axs.ravel(), aug_batch):\n",
                "    ax.imshow(img)\n",
                "    ax.axis('off')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Data augmentation is an important topic and it will be revisited several times throughout the course!"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=\"val_split\" class='exercise'\u003e\u003cb\u003eCreating a Validation Set\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "We can see from `ds_info` above that this dataset had predefined train and test sets. We loaded both.\n",
                "\n",
                "Some TF datasets also have a validation set. Others have only train. We'd like to use a validation set while training our model. And in situations where your data set only has a train set it will be important to know how to create new splits. As of Tensorflow 2.10 this is made easy with the `tf.keras.utils.split_dataset()` methods which works similar to the train_test_split in SKLearn."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "left_ds, right_ds = keras.utils.split_dataset(ds_train, left_size=0.8)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "left_ds.cardinality() + right_ds.cardinality() == ds_train.cardinality()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003ca id='transfer_learning'\u003e\u003c/a\u003e\n",
                "## Transfer Learning [^](#contents \"Back to Contents\")\n",
                "\n",
                "Here we can see an example of transfer learning where we use the pretrained MobileNet model as the base of our classifier."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=\"mobilenet\" class='exercise'\u003e\u003cb\u003eMobileNet\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "ds_train, ds_info = tfds.load('horses_or_humans', split='train', as_supervised=True, with_info=True)\n",
                "ds_test = tfds.load('horses_or_humans', split='test', as_supervised=True)\n",
                "ds_train, ds_val = keras.utils.split_dataset(ds_train, left_size=0.8)\n",
                "\n",
                "def preprocess(image, label):\n",
                "    image = tf.image.resize(image, (224, 224))\n",
                "    image = tf.keras.applications.mobilenet.preprocess_input(image)\n",
                "    return image, label\n",
                "\n",
                "ds_train = ds_train.map(preprocess).map(augment).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
                "ds_val = ds_val.map(preprocess).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
                "ds_test = ds_test.map(preprocess).batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),\n",
                "                                                include_top=False,\n",
                "                                                weights='imagenet')\n",
                "\n",
                "# Freeze the base model's layers to prevent them from being updated during training\n",
                "base_model.trainable = False\n",
                "\n",
                "# Add custom layers on top of MobileNet\n",
                "x = base_model.output\n",
                "x = keras.layers.GlobalAveragePooling2D()(x)\n",
                "# Add a fully-connected layer\n",
                "x = keras.layers.Dense(1024, activation='relu')(x)\n",
                "# Add a logistic layer for binary classification\n",
                "predictions = keras.layers.Dense(1, activation='sigmoid')(x)\n",
                "\n",
                "# This is the model we will train\n",
                "model = Model(inputs=base_model.input, outputs=predictions)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.compile(optimizer=optimizers.Adam(learning_rate=0.0001),\n",
                "              loss='binary_crossentropy',\n",
                "              metrics=['accuracy'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.fit(ds_train,\n",
                "          validation_data=ds_val,\n",
                "          epochs=10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.evaluate(ds_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
