{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \u003cimg style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"\u003e CS1090B Introduction to Data Science\n",
                "\n",
                "## Lab 5:  Feed Forward Neural - Regularization\n",
                "\n",
                "**Harvard University**\u003cbr/\u003e\n",
                "**Spring 2025**\u003cbr/\u003e\n",
                "**Instructors**: Pavlos Protopapas, Natesh Pillai, and Chris Gumb\u003cbr/\u003e\n",
                "\u003cbr/\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "#RUN THIS CELL \n",
                "import requests\n",
                "from IPython.core.display import HTML\n",
                "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
                "HTML(styles)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Table of Contents\n",
                "- Regularization\n",
                "    - Early Stopping (Keras Callbacks)\n",
                "    - Norm Penalties (L1 \u0026 L2)\n",
                "    - Dropout\n",
                "    - Data Augmentation (Gaussian Noise)\n",
                "- Batchnorm \n",
                "- Data Generators (more data augmentation!)\n",
                "- Activity (Fashion MNIST regularization)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import copy\n",
                "import operator\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from PIL import Image\n",
                "import random as rn\n",
                "import seaborn as sns\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
                "import tensorflow as tf\n",
                "\n",
                "# This will hide oneDNN notifications from Tensorflow\n",
                "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# You can adjust the notebook width if you like\n",
                "# from IPython.display import display, HTML\n",
                "# display(HTML(\"\u003cstyle\u003e.container { width:75% !important; }\u003c/style\u003e\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Recite the sacred reproducibility incantation\n",
                "os.environ['PYTHONHASHSEED'] = '0'\n",
                "# os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
                "tf.random.set_seed(109)\n",
                "np.random.seed(109)\n",
                "rn.seed(109)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Overfitting"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the previous lab on optimizers we were running our examples on an oversimplified dataset where all the datapoints are right on top of the true generating function. But the real world is noisy!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv('data/lorentz_noise_set2.csv')\n",
                "df = df.sample(frac=1, random_state=109) # shuffle DataFrame!\n",
                "\n",
                "scaler = StandardScaler()\n",
                "df['x_std'] = scaler.fit_transform(df[['x']])\n",
                "x_train, x_test, y_train, y_test = train_test_split(df.x_std, df.y, train_size=0.7, random_state=109)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_lorentz(df, test_idx, ax=None):\n",
                "    if ax is None:\n",
                "        ax = plt.gca()\n",
                "    train_mask = np.ones(df.shape[0], dtype=bool)\n",
                "    train_mask[test_idx] = False\n",
                "    ax.scatter(df.x[train_mask],df.y[train_mask], c='b', label='train data')\n",
                "    ax.scatter(df.x[~train_mask],df.y[~train_mask], c='orange', marker='^', label='test data')\n",
                "    ax.set_xlabel('$\\omega$')\n",
                "    ax.set_ylabel('$\\epsilon$')\n",
                "    ax.legend()\n",
                "\n",
                "plot_lorentz(df, x_test.index)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "How does the best performing model from the previous lab fare on this more realistic dataset?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.layers import Input, Dense, BatchNormalization\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.optimizers import Adam"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper function for plotting training history\n",
                "def plot_history(model, title=None, ax=None):\n",
                "    if ax is None:\n",
                "        ax = plt.gca()\n",
                "    ax.plot(model.history.history['loss'], label='train')\n",
                "    ax.plot(model.history.history['val_loss'], label='validation')\n",
                "    ax.set_xlabel('epoch')\n",
                "    ax.set_ylabel('MSE')\n",
                "    best_loss = np.nanmin(model.history.history['val_loss'])\n",
                "    ax.axvline(np.nanargmin(model.history.history['val_loss']),\n",
                "                c='k', ls='--',\n",
                "                label=f'best val loss = {best_loss:.2f}')\n",
                "    ax.legend()\n",
                "    ax.set_title(title)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "%%time\n",
                "inputs = Input(shape=(1,))\n",
                "x = Dense(64, activation='relu')(inputs)\n",
                "x = Dense(128, activation='relu')(x)\n",
                "outputs = Dense(1, activation='linear')(x)\n",
                "NN2 = Model(inputs=inputs, outputs=outputs)\n",
                "NN2.compile(optimizer=Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),\n",
                "            loss='mse')\n",
                "NN2.fit(x_train, y_train, \n",
                "        validation_data=(x_test, y_test),\n",
                "        batch_size=32,\n",
                "        epochs=1000,\n",
                "        verbose=0)\n",
                "\n",
                "plot_history(NN2);"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There are clear signs of overfitting as the validation error starts to diverge from the train error. Any \"improvement\" seen with respect to the training data after a certain point no longer generalizes. And after a while, we actually start to see the validation loss increasing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the overfit predictions\n",
                "plot_lorentz(df, test_idx=x_test.index)\n",
                "\n",
                "def plot_predictions(model):\n",
                "    x_lin = np.linspace(df.x.min(), df.x.max(), (500)).reshape(-1,1)\n",
                "    y_hat = model.predict(scaler.transform(pd.DataFrame(x_lin, columns=['x'])))\n",
                "    ax = plt.gca()\n",
                "    ax.plot(x_lin, y_hat);\n",
                "\n",
                "plot_predictions(NN2)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And here we can see the model predictions jump around as it tries to fit the sparse and noisy points in the training data.\\\n",
                "Luckily we have several tools at our disposal for addressing overfitting in neural networks."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Regularization"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise'\u003e\u003cb\u003eEarly Stopping (Keras Callbacks)\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "Training a NN can take a long time. We should checkpoint our model so we don't lose progress and stop early if we don't see improvement to help save on training time. This motivates [Keras Callbacks](https://keras.io/api/callbacks/).\n",
                "\n",
                "A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[`EarlyStopping`](https://keras.io/api/callbacks/early_stopping/) allows us to end the training process if we haven't seen an improvment in some loss or metric of our choice (`monitor`) for some number of epochs (`patience`). This even gives us the option of reverting back to the best state of the network (according to the chosen metric) when training ends."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# monitor validation loss and stop training if it does not improve for 75 epochs\n",
                "# once training stops, restore the weights with the lowest validation loss\n",
                "es = EarlyStopping(monitor='val_loss', patience=75, restore_best_weights=True, verbose=1)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[`ModelCheckpoint`](https://keras.io/api/callbacks/model_checkpoint/) lets us save our models to disk periodically. For example, you might want to update your saved model file each time there is an improvement. Just imagine how sad it would be if your kernel died or there was some other failure thousands of epochs into the training process. Checkpointing can prevent these tragedies, but it does slow down training while saving the weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "mc = ModelCheckpoint('data/models', monitor='val_loss', save_best_only=True, save_weights_only=False)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also execute arbitrary functions at different the start/end of a batch/epoch using the [`LambdaCallback`](https://keras.io/api/callbacks/lambda_callback/)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use a lambda callback to print the epoch number every 50 epochs.\n",
                "lcall = LambdaCallback(on_epoch_end=lambda epoch, logs: print(f'epoch {epoch}') if epoch % 50 == 0 else None)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Any callbacks we wish to use during training are passed as a list to the `fit` function's `callbacks` argument."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "inputs = Input(shape=(1,))\n",
                "x = Dense(64, activation='relu')(inputs)\n",
                "x = Dense(128, activation='relu')(x)\n",
                "outputs = Dense(1, activation='linear')(x)\n",
                "NN_es = Model(inputs=inputs, outputs=outputs)\n",
                "NN_es.compile(optimizer=Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),\n",
                "            loss='mse')\n",
                "\n",
                "NN_es.fit(x_train, y_train, \n",
                "        validation_data=(x_test, y_test),\n",
                "        batch_size=32,\n",
                "        epochs=1000,\n",
                "        callbacks=[es, lcall],\n",
                "        verbose=0)\n",
                "plot_history(NN_es, title=\"Early Stopping\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_lorentz(df, test_idx=x_test.index)\n",
                "plot_predictions(NN_es)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Weight Decay"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.regularizers import L1, L2"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also add a penalty term to our loss function that penalizes the model based on the magnitutes of its weights. This forces the network to find a balance between model complexity and goodness-of-fit to the training data, reducing the tendency to overfit."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**$L_2$ Regularization**\n",
                "\n",
                "Penalty based on the square of network weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "\n",
                "# Regularize the kernel and bias using L2 regularization coefficient l2=0.005\n",
                "# your code here\n",
                "kernel_regularizer = ...\n",
                "# your code here\n",
                "bias_regularizer = ...\n",
                "\n",
                "inputs = Input(shape=(1,))\n",
                "x = Dense(64, activation='relu',\n",
                "          kernel_regularizer=kernel_regularizer,\n",
                "          bias_regularizer=bias_regularizer)(inputs)\n",
                "x = Dense(128, activation='relu',\n",
                "          kernel_regularizer=kernel_regularizer,\n",
                "          bias_regularizer=bias_regularizer)(x)\n",
                "outputs = Dense(1, activation='linear')(x)\n",
                "NN1 = Model(inputs=inputs, outputs=outputs)\n",
                "NN1.compile(optimizer=Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999), loss='mse')\n",
                "\n",
                "NN1.fit(x_train, y_train, \n",
                "        validation_data=(x_test, y_test),\n",
                "        batch_size=32,\n",
                "        epochs=2000,\n",
                "        callbacks=[es, lcall],\n",
                "        verbose=0)\n",
                "plot_history(NN1)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**$L_1$ Regularization**\n",
                "\n",
                "Penalty based on the absolute value of model weights. Like LASSO in linear regression, this tends to push some weights to zero, resulting in \"sparsity\" (i.e., some nodes are \"off\" in the network)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "\n",
                "# Regularize the kernel and bias using L1 regularization coefficient l1=0.005\n",
                "# your code here \n",
                "kernel_regularizer = ...\n",
                "# your code here \n",
                "bias_regularizer = ...\n",
                "\n",
                "inputs = Input(shape=(1,))\n",
                "x = Dense(64, activation='relu',\n",
                "          kernel_regularizer=kernel_regularizer,\n",
                "          bias_regularizer=bias_regularizer)(inputs)\n",
                "x = Dense(128, activation='relu',\n",
                "          kernel_regularizer=kernel_regularizer,\n",
                "          bias_regularizer=bias_regularizer)(x)\n",
                "outputs = Dense(1, activation='linear')(x)\n",
                "NN_L1 = Model(inputs=inputs, outputs=outputs)\n",
                "NN_L1.compile(optimizer=Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),\n",
                "            loss='mse')\n",
                "\n",
                "NN_L1.fit(x_train, y_train, \n",
                "        validation_data=(x_test, y_test),\n",
                "        batch_size=32,\n",
                "        epochs=2000,\n",
                "        callbacks=[es, lcall],\n",
                "        verbose=0)\n",
                "plot_history(NN_L1, title='L1 Regularized Model')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### [Dropout](https://keras.io/api/layers/regularization_layers/dropout/)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.layers import Dropout"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[tensorflow.keras.layers.Dropout](https://keras.io/api/layers/regularization_layers/dropout/)\n",
                "\n",
                "At each batch during training time, the Dropout layer randomly sets input units to 0 with probability `rate`. For example, `rate=0.25` would cause any input to the layer to have a 25% chance of being set to zero. \n",
                "\n",
                "Note that the Dropout layer only applies during training and no values are dropped during inference. \n",
                "\n",
                "Even if individual weight magnitudes are constrained with methods like weight decay above, many weights can \"conspire\" within a network to collectively have a large effect. Dropout helps prevent this because no weight can \"rely\" on any other for its affect as it may be turned off. \n",
                "\n",
                "You can also think of dropout as turning your network into a aggregated ensemble of smaller neural networks! "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "dropout_rate = 0.05\n",
                "inputs = Input(shape=(1,))\n",
                "x = Dense(64, activation='relu')(inputs)\n",
                "# Pass the data through a dropout layer with the rate set to dropout_rate\n",
                "# your code here\n",
                "...\n",
                "x = Dropout(rate=dropout_rate)(x)\n",
                "x = Dense(128, activation='relu')(x)\n",
                "# Use dropout again before the output layer\n",
                "# your code here\n",
                "...\n",
                "outputs = Dense(1, activation='linear')(x)\n",
                "NN_do = Model(inputs=inputs, outputs=outputs)\n",
                "NN_do.compile(optimizer=Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),\n",
                "            loss='mse')\n",
                "\n",
                "NN_do.fit(x_train, y_train, \n",
                "        validation_data=(x_test, y_test),\n",
                "        batch_size=32,\n",
                "        epochs=2000,\n",
                "        callbacks=[es, lcall],\n",
                "        verbose=0)\n",
                "plot_history(NN_do, title='Dropout Model')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Batch Normalization (Batchnorm) \n",
                "\n",
                "When we have inputs on different scales, we are advised to standardize them to put everything on the same scale.\\\n",
                "But now consider that from the perspective of the 2nd layer in your network, the outputs of the first layer are its input features. Should these be standardized for the same reasons we standardize the input to the first layer?\n",
                "\n",
                "Batch Normalization extends the idea of feature rescaling to (potentially) all the layers in your network.\n",
                "\n",
                "The \"batch\" in the name refers to the fact the the mean and standard deviation used for the standardization are \"noisy\" approximations calculated from the current mini-batch. This \"noisiness\" can have a *slight* regularization effect.\n",
                "\n",
                "More importantly, it helps keep the distribution of input values going into a given layer from shifting around. Your layer is trying to learn weights to map its input to the appropriate input, but this is made very difficult if the input values look wildly different after weights in the previous layers are also updated; it's like trying to hit a moving target.\n",
                "\n",
                "Batchnorm is normally placed *between* the affine transformation of a neuron and its activation function.\n",
                "A batchnorm layer has two trainable parameters which allow the network to learn what the ideal mean and standard deviation are for the data to possess before being passed through the activation function. Consider why this might be important for certain activation functions like relu or sigmoid.\n",
                "\n",
                "If we want to put a batchnorm layer before our activation functions then we can forego specifying an activation in the `Dense` layer and instead use an `Activation` layer!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.layers import Activation, BatchNormalization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "dropout_rate = 0.05\n",
                "activation='relu'\n",
                "inputs = Input(shape=(1,))\n",
                "x = Dense(64, activation=None)(inputs)\n",
                "\n",
                "x = BatchNormalization()(x)\n",
                "x = Activation(activation)(x)\n",
                "x = Dropout(dropout_rate, seed=109)(x)\n",
                "for i, n_nodes in enumerate([16]*12):\n",
                "    x = Dense(n_nodes, activation=None)(x)\n",
                "    x = BatchNormalization()(x)\n",
                "    x = Activation(activation)(x)\n",
                "    x = Dropout(dropout_rate, seed=209+i)(x)\n",
                "outputs = Dense(1, activation='linear')(x)\n",
                "NN_bn = Model(inputs=inputs, outputs=outputs)\n",
                "display(NN_bn.summary())\n",
                "NN_bn.compile(optimizer=Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),\n",
                "            loss='mse')\n",
                "\n",
                "NN_bn.fit(x_train, y_train, \n",
                "        validation_data=(x_test, y_test),\n",
                "        batch_size=128,\n",
                "        epochs=3000,\n",
                "        callbacks=[EarlyStopping(monitor='val_loss', patience=200, restore_best_weights=True, verbose=1),\n",
                "                   lcall],\n",
                "        verbose=0)\n",
                "plot_history(NN_bn, title='Batchnorm Model')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data Augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.layers import GaussianNoise"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The easiest way to fix overfitting is to use more data. This isn't always an option, but we can often \"fake it\" with some success. The idea behind **data augmentation** is to produce variations of your existing data that are still close enough to real examples. In this way the model becomes less sensitive to any of the idiosyncratic features of your particular training examples and instead will learn what is essential and common across all your generated data.\n",
                "\n",
                "Perhaps the easiest form of data augmentation is simply adding a bit of gaussian noise to your data points.\\\n",
                "This is made very easy with Keras's [`GaussianNoise`](https://keras.io/api/layers/regularization_layers/gaussian_noise/) layer. Just specify a stddev for the noise distribution. And keep in mind that it can take a random seed!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "noise_stddev = 0.02\n",
                "inputs = Input(shape=(1,))\n",
                "\n",
                "# Add gaussian noise to your input data here\n",
                "# your code here\n",
                "...\n",
                "x = Dense(64, activation='relu')(x)\n",
                "x = Dense(128, activation='relu')(x)\n",
                "outputs = Dense(1, activation='linear')(x)\n",
                "NN_da = Model(inputs=inputs, outputs=outputs)\n",
                "NN_da.compile(optimizer=Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),\n",
                "            loss='mse')\n",
                "\n",
                "NN_da.fit(x_train, y_train, \n",
                "        validation_data=(x_test, y_test),\n",
                "        batch_size=32,\n",
                "        epochs=2000,\n",
                "        callbacks=[es, lcall],\n",
                "        verbose=0)\n",
                "plot_history(NN_da, title='Model /w Data Augmentation (Gaussian Noise)')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2 - Classification with Neural Networks\n",
                "\n",
                "In this 2nd half of the lab we will be working with the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist).\n",
                "\u003cimg src=\"https://4.bp.blogspot.com/-OQZGt_5WqDo/Wa_Dfa4U15I/AAAAAAAAAUI/veRmAmUUKFA19dVw6XCOV2YLO6n-y_omwCLcBGAs/s400/out.jpg\" width=\"400px\" /\u003e\n",
                "\n",
                "Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
                "\n",
                "We will be using a very small fraction of the dataset. The goal is to first overfit a NN to this classification task and then use the tools explored in the 1st half of the notebook to regularize and improve the model through hyperparameter tweaking.\n",
                "\n",
                "But first, we will explore a new option for **image data augmentation** through the use of Keras's `ImageDataGenerator` ojects which may be useful to us."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## DataGenerators in Keras"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This section shows how to build data generators from a collection of image files to be used for training a Keras model. The process of creating the file structure required by data generators is normally quite tedious. The code below can be adapted and reused for your own projects to save you a lot of time!"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[(keras ImageGenerator documentation)](https://keras.io/preprocessing/image/)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Fashion-MNIST and other pre-loaded dataset are formatted in a way that is almost ready for feeding into a model. But what about your average image files? They should be appropriately preprocessed into floating-point tensors before being fed into the network."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Imagine you have a bunch of dog and cat images you want to prepare for a Keras model using a `ImageDataGenerator`.\u003c/br\u003e\n",
                "We need to create:\n",
                "1. train, validation, and test directories, each containing a subset of the images.\n",
                "2. separate cat and dog directories _within_ train, validation, and test.\n",
                "\n",
                "Number 2 is necessary because the Keras ImageDataGenerator infers the class label from the subdirectory the image resides in.\n",
                "\n",
                "The directory structure should then look like this:"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```\n",
                " dogs_vs_cats\n",
                " ├── test\n",
                " │   ├── cats\n",
                " │   └── dogs\n",
                " ├── train\n",
                " |   ├── cats\n",
                " |   └── dogs\n",
                " └── validation\n",
                "     ├── cats\n",
                "     └── dogs\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This is what we will do for the `Fashion-MNIST` data. It would be a lot of work to do it manually, especially with 10 classes! But it isn't to difficult to take care of all of this programatically."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**A Note About JupyterHub:**\n",
                "\n",
                "One weakness of the JupyterHub server is slow disk read \u0026 write speeds. So if you decide to use the Datagenerators (which requires that files be read from disk each batch) then you may find it runs just as fast (if not faster) on your local machine. We'll see an alternative to Datagenerators later in the course."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load entire Fashion-MNIST dataset from Tensorflow\n",
                "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "These are the 10 class labels and the integer they map to in `y_train` and `y_test`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fashion-MNIST class labels\n",
                "labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
                "label2idx = {label: idx for idx, label in enumerate(labels)}\n",
                "label2idx"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here's some examples from the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fashion-MNIST examples\n",
                "fig, axs = plt.subplots(3,5, figsize=(9,7))\n",
                "for i, ax in enumerate(axs.ravel()):\n",
                "    ax.imshow(x_train[i], cmap=plt.cm.gray)\n",
                "    ax.set_xticks(())\n",
                "    ax.set_yticks(())\n",
                "    ax.set_title(labels[y_train[i]])\n",
                "plt.tight_layout()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset size\n",
                "x_train.shape, x_test.shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Because our goal is to first overfit (and have a model we can train multiple times as we experiment during the lab!) we will discard most of the training data. We'll do stratified splits to avoid biasing our sample. The classes are equally represented in the original dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Throw away most of the training data to make things more difficult\n",
                "# We'll also toss most of the test data (writing it all to disk is slow on JupyterHub anyway!)\n",
                "_, x_test, _, y_test = train_test_split(x_test, y_test, test_size=0.10, stratify=y_test)\n",
                "x_train, _, y_train, _ = train_test_split(x_train, y_train, train_size=0.02, stratify=y_train)\n",
                "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample size\n",
                "x_train.shape, x_val.shape, x_test.shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The `PIL` module (Python Image Library) is useful for taking the numpy arrays that currently represent the images and writing them to disk as jpeg files. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from PIL import Image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_DIR = 'data/fashionMNIST' # create path name for data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "data = {'train': (x_train, y_train),\n",
                "        'validation': (x_val, y_val),\n",
                "        'test': (x_test, y_test)}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# Construct directory structure and write image files to disk\n",
                "if not os.path.isdir(DATA_DIR):\n",
                "    os.makedirs(DATA_DIR, exist_ok=True)\n",
                "\n",
                "    for split, (x, y) in data.items():\n",
                "        for label in labels:\n",
                "            label_s = label.replace('/','_')\n",
                "            new_dir = os.path.join(DATA_DIR, split, label_s)\n",
                "            os.makedirs(new_dir, exist_ok=True)\n",
                "            target = label2idx[label]\n",
                "            cur_images = x[y==target]\n",
                "            for i in range(cur_images.shape[0]):\n",
                "                im = Image.fromarray(cur_images[i]).convert(\"L\")\n",
                "                filename = f\"{label_s}_{split}_{i:04d}\"\n",
                "                im.save(f\"{DATA_DIR}/{split}/{label_s}/{filename}.jpeg\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# number of images in each subdir\n",
                "for dir_name in data.keys():\n",
                "    print(dir_name)\n",
                "    for label in labels:\n",
                "        label_dir = label.replace('/','_')\n",
                "        print('\\t'+label_dir, len(os.listdir(DATA_DIR + '/' + dir_name + '/' + label_dir)))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise'\u003e\u003cb\u003eCreate the Generators\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "Now that we have the data in the correct directory structure we can create the data generators.\u003c/br\u003e\n",
                "Note that we will have _multiple_ generators, one for each split directory.\u003c/br\u003e\n",
                "First we create a a main data generator object, `datagen`. This can be a given a wide range of arguments which can be used to preprocess the images it generates.\u003c/br\u003e\n",
                "For right now we will only use the `rescale` argument to normalize all pixel values to between 0 and 1. (Remember that image data is stored with pixels shaded from 0 to 255.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The `ImageDataGenerator` can automate the scaling of normalizing the image pixel values to [0,1]. This is just a first glimpse of how these objects can help with a preprocessing and training pipeline!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "datagen = ImageDataGenerator(rescale=1./255)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we use `datagen`'s `flow_from_directory` method to create the 3 generators: `traingen`, `valgen`, and `testgen`.\u003c/br\u003e\n",
                "The need to be given the `directory` which they will use as their image source, a `target_size` to resize all images to (e.g., (14,14)), a `batch_size`, and `class_mode` to instruct the generator on how to interpret the label folders. We should probably also set `shuffle = False` in the test generator so it produces the same images in the same order everytime it is used. We also set `color_mode='grayscale'`. Otherwise, the default is to output color images which would be of shape `(28, 28, 3)` because they have 3 color channels (more on that next lab!)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_size = 32\n",
                "target_size = (28, 28) # generator can resize all images if we want\n",
                "# this is the augmentation configuration we will use for training\n",
                "\n",
                "traingen = datagen.flow_from_directory(\n",
                "                                DATA_DIR+'/train',  # this is the target directory\n",
                "                                target_size=target_size,  # all images will be resized to 150x150\n",
                "                                batch_size=batch_size,\n",
                "                                class_mode='categorical',  # since we use categorical_crossentropy loss, we need binary labels\n",
                "                                 color_mode='grayscale')\n",
                "valgen = datagen.flow_from_directory(\n",
                "                                DATA_DIR+'/validation',\n",
                "                                target_size=target_size, \n",
                "                                batch_size=batch_size,\n",
                "                                class_mode='categorical',\n",
                "                                color_mode='grayscale') \n",
                "\n",
                "testgen = datagen.flow_from_directory(\n",
                "                        DATA_DIR+'/test',  \n",
                "                        target_size=target_size, \n",
                "                        batch_size=batch_size,\n",
                "                        shuffle = False,\n",
                "                        class_mode='categorical',\n",
                "                        color_mode='grayscale') "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let’s look at the output of one of these generators to see how `target_size` and `batch_size` affects the output. Note that the generator yields these batches indefinitely: it loops endlessly over the images in the target folder. For this reason, you need to break the iteration loop at some point if using a `for` loop. Better still, you can use the built-in `next` function to return a single element from the generator."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_batch, labels_batch = next(traingen)\n",
                "print('data batch shape:', data_batch.shape)\n",
                "print('labels batch shape:', labels_batch.shape)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise'\u003e\u003cb\u003eConstruct a Classifier NN\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "Let's built our first attempt at a clothing classifier and try to overfit.\n",
                "\n",
                "Note that Keras has a `Flatten` layer! We can use this to automatically turm input images into 1D arrays.\\\n",
                "(We'll see how to handle 2D input in future lectures and labs) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import  Activation, Input, BatchNormalization, Dense, Dropout, Flatten, GaussianNoise\n",
                "from tensorflow.keras.optimizers import Adam, SGD"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "# Overfit Fashion-MNIST Classifier\n",
                "input_dim = data_batch.shape[1:]\n",
                "n_classes = labels_batch.shape[-1]\n",
                "inputs = Input(shape=(input_dim))\n",
                "\n",
                "# Flatten the inputs here to turn them into 1D arrays.\n",
                "flat = Flatten()(inputs) \n",
                "\n",
                "x = Dense(256, activation=None, kernel_initializer='he_uniform')(flat)\n",
                "x = BatchNormalization()(x)\n",
                "x = Activation('relu')(x)\n",
                "x = Dense(128, activation=None, kernel_initializer='he_uniform')(x)\n",
                "x = BatchNormalization()(x)\n",
                "x = Activation('relu')(x)\n",
                "x = Dense(64, activation=None, kernel_initializer='he_uniform')(x)\n",
                "x = BatchNormalization()(x)\n",
                "x = Activation('relu')(x)\n",
                "x = Dense(32, activation=None, kernel_initializer='he_uniform')(x)\n",
                "x = BatchNormalization()(x)\n",
                "x = Activation('relu')(x)\n",
                "x = Dense(n_classes, activation=None, kernel_initializer='he_uniform')(x)\n",
                "x = BatchNormalization()(x)\n",
                "outputs = Activation('softmax')(x)\n",
                "NN = Model(inputs, outputs)\n",
                "NN.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
                "NN.summary()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise'\u003e\u003cb\u003eKeras Callbacks\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "\n",
                "We'll use early stopping with short patience so we can call it quits early when it looks like we are overfitting so we can save time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback\n",
                "\n",
                "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
                "callbacks = [es]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise'\u003e\u003cb\u003eFit Model with Generator\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let’s fit the model to the data using the generator. You can use `fit` as before but this time you will pass it generators rather than dataframes or numpy arrays.  \n",
                "\n",
                "Because the data is being generated endlessly, the Keras model needs to know how many samples to draw from the generator before declaring an epoch over. This is the role of the `steps_per_epoch` argument: after having drawn steps_per_epoch batches from the generator—that is, after having run for steps_per_epoch gradient descent steps - the fitting process will go to the next epoch. \n",
                "\n",
                "When using `fit`, you can pass a validation_data argument, much as with the fit method. It’s important to note that this argument is allowed to be a data generator, but it could also be a tuple of Numpy arrays. If you pass a generator as validation_data, then this generator is expected to yield batches of validation data endlessly; thus you should also specify the validation_steps argument, which tells the process how many batches to draw from the validation generator for evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "%%time\n",
                "history = NN.fit(\n",
                "        traingen,\n",
                "        steps_per_epoch=traingen.samples//traingen.batch_size,\n",
                "        epochs=30,\n",
                "        validation_data=valgen,\n",
                "        validation_steps=valgen.samples//valgen.batch_size,\n",
                "        callbacks=callbacks,\n",
                "        workers=-1,\n",
                "        use_multiprocessing=True,\n",
                "        verbose=1)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise'\u003e\u003cb\u003eEvaluate the Model\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "NN.evaluate(traingen)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "NN.evaluate(valgen)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "NN.evaluate(testgen)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let’s plot the loss and accuracy of the model over the training and validation data during training:"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise'\u003e\u003cb\u003ePlot the Training History\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axs = plt.subplots(1,2, figsize=(10,4))\n",
                "print(f\"Final Val Acc: {history.history['val_accuracy'][-1]:.2f}\")\n",
                "axs[0].plot(history.history['accuracy'])\n",
                "axs[0].plot(history.history['val_accuracy'])\n",
                "axs[0].set_title('model accuracy')\n",
                "axs[0].set_ylabel('accuracy')\n",
                "axs[0].set_xlabel('epoch')\n",
                "axs[0].legend(['train', 'validation'], loc='upper left')\n",
                "\n",
                "# summarize history for loss\n",
                "axs[1].plot(history.history['loss'])\n",
                "axs[1].plot(history.history['val_loss'])\n",
                "axs[1].set_title('model loss')\n",
                "axs[1].set_ylabel('loss')\n",
                "axs[1].set_xlabel('epoch')\n",
                "axs[1].legend(['train', 'validation'], loc='upper left');"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Now with Data Augmentation\n",
                "\n",
                "We saw in the lecture on regularization that we can slightly alter our training data to simulate a larger dataset. This means our model is less likely to overfit as it would have to \"memorize\" more data.\n",
                "\n",
                "Take a look again at the [ImageDataGenerator Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) to see what choices we have for changing our data. Are all changes appropriate here?\u003c/br\u003e\n",
                "Create a new data generator `datagen_aug` that preprocesses images with your chosen changes. Then we only need to make a new `traingen_aug` using `flow_from_directory`.\n",
                "\n",
                "The `preprocessing_function` argument can be passed an arbitrary function to act on images pulled from the generator. Here we provide an example of a function to add Gaussian noise."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise'\u003e\u003cb\u003eCreate Augmenting Generator\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# your code here\n",
                "def add_noise(img):\n",
                "    VARIABILITY = 100 # customize this\n",
                "    deviation = VARIABILITY*np.random.random()\n",
                "    noise = np.random.normal(0, deviation, img.shape)\n",
                "    img += noise\n",
                "    img = np.clip(img, 0., 255.)\n",
                "    return img\n",
                "\n",
                "datagen_aug = ImageDataGenerator(\n",
                "      rescale=1./255,\n",
                "      ## customize these and other parameters\n",
                "      # rotation_range=0,\n",
                "      # width_shift_range=0,\n",
                "      # height_shift_range=0,\n",
                "      # shear_range=0,\n",
                "      zoom_range=0.05,\n",
                "      preprocessing_function=add_noise,\n",
                "      horizontal_flip=True,\n",
                "      fill_mode='nearest',\n",
                ")\n",
                "\n",
                "# We don't want to augment the validation (or test) data\n",
                "traingen_aug = datagen_aug.flow_from_directory(\n",
                "    DATA_DIR+'/train',\n",
                "    target_size=target_size,\n",
                "    batch_size=batch_size,\n",
                "    class_mode='categorical',\n",
                "    color_mode='grayscale')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "These are just a few of the options available (for more, see the Keras documentation). \n",
                "Let’s quickly go over this code:\n",
                "\n",
                "- rotation_range is a value in degrees (0–180), a range within which to randomly rotate pictures.\n",
                "- width_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\n",
                "- shear_range is for randomly applying shearing transformations.\n",
                "- zoom_range is for randomly zooming inside pictures.\n",
                "- horizontal_flip is for randomly flipping half the images horizontally—relevant when there are no assumptions of - horizontal asymmetry (for example, real-world pictures).\n",
                "- fill_mode is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift. \n",
                "\n",
                "Let’s look at some augmented images generated from a single source image."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Examples of augmented images\n",
                "from tensorflow.keras.preprocessing import image\n",
                "train_gb_dir = os.path.join(DATA_DIR, 'train', 'Shirt')\n",
                "fnames = [os.path.join(train_gb_dir, fname) for fname in os.listdir(train_gb_dir)]\n",
                "img_path = fnames[3] # Chooses one image to augment\n",
                "img = image.load_img(img_path, target_size=target_size)\n",
                "# Reads the image and resizes it\n",
                "x = image.img_to_array(img) # Converts it to a Numpy array\n",
                "x = x.reshape((1,) + x.shape) # Reshapes it \n",
                "i = 0\n",
                "for batch in datagen_aug.flow(x, batch_size=1):\n",
                "    plt.figure(i)\n",
                "    imgplot = plt.imshow(image.array_to_img(batch[0]).convert('L'), cmap='gray')\n",
                "    i += 1\n",
                "    if i % 4 == 0:\n",
                "        break\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here you can generate a random augmented image."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample from augmented data generator\n",
                "example = next(traingen_aug)[0][0]\n",
                "print(example.shape)\n",
                "plt.imshow(example, cmap='gray');"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you train a new network using this data-augmentation configuration, the network will never see the same input twice. But the inputs it sees are obviously very related, because they come from a small number of original images—you can’t produce new information, you can only remix existing information. As such, this may not be enough to completely get rid of overfitting. To further fight overfitting and improve the model, use all the tools at your disposal: activation functions, initializers, optimizers, regularization techniques, callbacks, and batchnorm!"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class='exercise'\u003e\u003cb\u003e2nd FFN Model\u003c/b\u003e\u003c/div\u003e\u003c/br\u003e\n",
                "To see the effect of your choices, you should make several attempts in which you do not change the overall model architecture (i.e., number of layers and nodes)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "# your code here\n",
                "data_batch, labels_batch = next(traingen_aug)\n",
                "print('data batch shape:', data_batch.shape)\n",
                "print('labels batch shape:', labels_batch.shape)\n",
                "\n",
                "input_dim = data_batch.shape[1:]\n",
                "n_classes = labels_batch.shape[-1]\n",
                "\n",
                "inputs = Input(shape=(input_dim))\n",
                "flat = Flatten()(inputs)\n",
                "x = ...\n",
                "...\n",
                "outputs = ...\n",
                "\n",
                "NN_aug = Model(inputs, outputs)\n",
                "NN_aug.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
                "NN_aug.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "%%time\n",
                "# your code here\n",
                "history_aug = NN_aug.fit(\n",
                "        ...,\n",
                "        steps_per_epoch=...,\n",
                "        epochs=100,\n",
                "        validation_data=...,\n",
                "        validation_steps=...,\n",
                "        callbacks=...,\n",
                "        verbose=1)\n",
                "\n",
                "# save model if needed\n",
                "NN_aug.save('data/gen_model_aug.keras')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axs = plt.subplots(1,2, figsize=(10,4))\n",
                "print(f\"Final Val Acc: {history_aug.history['val_accuracy'][-1]:.2f}\")\n",
                "axs[0].plot(history_aug.history['accuracy'])\n",
                "axs[0].plot(history_aug.history['val_accuracy'])\n",
                "axs[0].set_title('model accuracy')\n",
                "axs[0].set_ylabel('accuracy')\n",
                "axs[0].set_xlabel('epoch')\n",
                "axs[0].legend(['train', 'validation'], loc='upper left')\n",
                "\n",
                "# summarize history for loss\n",
                "axs[1].plot(history_aug.history['loss'])\n",
                "axs[1].plot(history_aug.history['val_loss'])\n",
                "axs[1].set_title('model loss')\n",
                "axs[1].set_ylabel('loss')\n",
                "axs[1].set_xlabel('epoch')\n",
                "axs[1].legend(['train', 'validation'], loc='upper left');"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "NN_aug.evaluate(traingen)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "NN_aug.evaluate(valgen)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "NN_aug.evaluate(testgen)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "How high are you able to get the validation accuracy? (Can you beat 83%?) Does it translate well to test accuracy after all your optimization and tuning?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
