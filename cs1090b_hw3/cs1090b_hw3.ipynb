{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"cs1090b_hw3.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmO29d4ux0Lq"
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> Data Science 2: Advanced Topics in Data Science \n",
    "## Homework 3: Convolutional Neural Networks\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2025**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Natesh Pillai, and Chris Gumb\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "TPTLfywkx0Lw",
    "outputId": "c2ea408b-93c1-428d-ce53-05cf88cad644"
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/\"\n",
    "    \"content/styles/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow_datasets as tfds\n",
    "except ImportError:\n",
    "    print(\"Installing tensorflow-datasets...\")\n",
    "    !pip install -q --no-warn-script-location tensorflow-datasets==4.9.4\n",
    "    print(\"Done!\\n‚ö†Ô∏è Please restart kernel for changes to take effect (Kernel > Restart)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HaPrtFjfx0L1",
    "outputId": "32fad8bb-f1f0-4f4a-9ec1-8aa53a6560b7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import zipfile\n",
    "import random\n",
    "import imageio\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import tarfile\n",
    "import gdown\n",
    "from PIL import Image\n",
    "import requests\n",
    "import scipy.ndimage as ndimage\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, GaussianNoise\n",
    "from tensorflow.keras.layers import Flatten, Input, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.regularizers import L1, L2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TF GPU check\n",
    "print(f\"tensorflow version {tf.__version__}\")\n",
    "print(f\"Available GPUs: \\n{tf.config.list_logical_devices('GPU')}\\n\")\n",
    "\n",
    "tf.random.set_seed(2266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure notebook runtime\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"background: lightsalmon; border: thin solid black; border-radius: 2px; padding: 5px\">\n",
    "\n",
    "### Instructions\n",
    "- To submit your notebook, follow the instructions given in on the Canvas assignment page.\n",
    "- Plots should be legible and interpretable *without having to refer to the code that generated them*. They should includelabels for the $x$- and $y$-axes as well as a descriptive title and/or legend when appropriate.\n",
    "- When asked to interpret a visualization, do not simply describe it (e.g., \"the curve has a steep slope up\"), but instead explain what you believe the plot *means*.\n",
    "- Autograding tests are mostly to help you debug. The tests are not exhaustive so simply passing all tests may not be sufficient for full credit.\n",
    "- The use of *extremely* inefficient or error-prone code (e.g., copy-pasting nearly identical commands rather than looping) may result in only partial credit.\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. Please get course staff approval before importing any additional 3rd party libraries.\n",
    "- Enable scrolling output on cells with very long output.\n",
    "- Feel free to add additional code or markdown cells as needed.\n",
    "- Ensure your code runs top to bottom without error and passes all tests by restarting the kernel and running all cells (note that this can take a few minutes). \n",
    "- **You should do a \"Restart Kernel and Run All Cells\" before submitting to ensure (1) your notebook actually runs and (2) all output is visible**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7AgX8bWx0L-"
   },
   "source": [
    "\n",
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook Contents\n",
    "\n",
    "- [**PART 1: Building a Basic CNN Classifier**](#part1)\n",
    "\n",
    "- [**PART 2: Regression with CNN**](#part2)\n",
    "\n",
    "- [**PART 3: Image Segmentation**](#part3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzGonykqx0L_"
   },
   "source": [
    "## About this Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L555gy_1x0L_"
   },
   "source": [
    "In this homework, we will explore Convolutional Neural Networks (CNNs).\n",
    "\n",
    "- In [PART 1](#part1), we will begin by building a CNN to classify CIFAR-10 images, a standard pedagogical problem.\n",
    "\n",
    "\n",
    "- Then, in [PART 2](#part2), we will then see that CNNs are great for more than just classifying our images! They can serve as image input processing for a variety of tasks, as we will show by training a network on the CelebA dataset to rotate images of faces upright.\n",
    "\n",
    "- Finally, [PART 3](#part3), we will look at a dataset of pet photos for image segmentation. This is a classification problem. But instead of assigning class probablities to the image as a whole, we assign class probabilities for each individual pixel, identifying what regions of the image belong to each class (e.g., 'pet' or 'background').  \n",
    "\n",
    "**IMPORTANT NOTES:**\n",
    "\n",
    "- Convolutional neural networks are computationally intensive.\n",
    "- **We highly recommend that you train your model on a system using GPUs. For this, we recommend using the [GPU-enabled Jupyter environment](https://ood.huit.harvard.edu/pun/sys/dashboard/batch_connect/sys/ood-jupyterlab-spack-conda/cs1090b/session_contexts/new) provided to you as part of this course.** \n",
    "- Models that take hours to train on CPUs can be trained in just minutes when using GPUs.\n",
    "- **To avoid getting frustrated by having to re-train your models every time you run your notebook, you should save your trained model weights for later use.** Model history dictionaries can also be saved to disk with `pickle` and checked with an `if not` condition. This is a great way to check if the model weights exist before training, preventing redundant retraining. Please, think of the penguins! üêß\n",
    "\n",
    "**KERNEL CRASHES:**\n",
    "\n",
    "If your kernel crashes as you attempt to train your model, please check the following items:\n",
    "- Models with too many parameters might not fit in GPU memory. Try reducing the size of your model.\n",
    "- A large `batch_size` will attempt to load too many images in GPU memory. Avoid using a very large batch size.\n",
    "- Avoid creating multiple copies of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOHqvz5Ux0ME",
    "tags": []
   },
   "source": [
    "<a id=\"part1\"></a>\n",
    "    \n",
    "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
    "\n",
    "# PART 1: Building a Basic CNN Model\n",
    "\n",
    "\n",
    "<a id=\"part1intro\"></a>\n",
    "\n",
    "## Overview\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "In this question, you will use Keras to create a convolutional neural network for predicting the \"type of object\" shown in each image from the [CIFAR-10](https://keras.io/datasets/#cifar10-small-image-classification) dataset. This dataset contains 50,000 32x32 colored training images and 10,000 test images of the same size, with a total of 10 classes, representing the \"type of object\" shown in each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<a id=\"q11\"></a>\n",
    "\n",
    "\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise'>\n",
    "    \n",
    "<b>1.1 Loading CIFAR-10 and Constructing the Model</b>\n",
    "<hr>\n",
    "<b>Q1.1.1 - Preprocessing</b>\n",
    "\n",
    "<a id=\"q111\"></a>\n",
    "\n",
    "Load the CIFAR-10 dataset from the `tensorflow.keras.datasets.cifar10` import shown at the top of this notebook. Perform any preprocessing of the data that might be required for this dataset.\n",
    "    \n",
    "You may choose to load cifar10 as either a numpy array or as a Tensorflow Dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21P6WuWVx0MJ",
    "outputId": "aa20b335-caf2-4fff-c849-62532498f931",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dX0OlDNcx0MF",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise'><b>Q1.1.2</b>\n",
    "\n",
    "<a id=\"q112\"></a>\n",
    "\n",
    "Construct a classification model architecture using a combination of the following layers: Conv2D, MaxPooling2D, Dense, Dropout and Flatten. The layers don‚Äôt necessarily need to be in this order, and you can use as many of these types of layers as you‚Äôd like. \n",
    "\n",
    "  - You may choose to construct your own implementation of a well-known architecture like AlexNet or VGG16, or you can create an architecture of your own devising.\n",
    "\n",
    "  - However, you MUST code the network yourself and not use a pre-written implementation. \n",
    "\n",
    "  - You must have multiple Conv2D layers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CvnFEcux0MK",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dX0OlDNcx0MF",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise'><b>Q1.2 Model parameter growth</b>\n",
    "\n",
    "<a id=\"q12\"></a>\n",
    "\n",
    "How does the number of total parameters change (e.g. linearly, exponentially, etc.) as the number of filters per layer increases (your model should have at least 2 Conv layers)? You can find this empirically by constructing multiple models with the same type of architecture and increasing the number of filters. Generate a plot showing the relationship and explain why it has this relationship. \n",
    "\n",
    "**HINT:** Completing this question is far easier if you write a function that generates your desired architecture in 1.1.2, with arguments that allow you to easily rebuild the architecture with varying numbers of filters per layer.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnS8zEOmx0MM",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dX0OlDNcx0MF",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id=\"q13\"></a>\n",
    "\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise'>\n",
    "\n",
    "<b>1.3 Choose a model, train and evaluate it</b>\n",
    "<hr>    \n",
    "<b>Q1.3.1</b>\n",
    "\n",
    "<a id=\"q131\"></a>\n",
    "\n",
    "Print the model summary for your chosen architecture, and report the total number of parameters. Then train your model using the CIFAR-10 dataset, and `validation_split=0.2`. You can choose to train your model for as long as you'd like, but you should aim for at least 10 epochs. Report your validation and test accuracies. You can achieve a test accuracy of over 75% in about 6 minutes of training.\n",
    "\n",
    "**Hint:** It would be helpful to add code which either saves your model to a local directory if it is the first time you're training it or loads your model if a saved file version currently exists in that directory. This will not only help save time when you rerun your notebook, but it will also ensure reproducible results in the rest of Part 1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHkmUJgix0MP",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise'><b>Q1.3.2</b>\n",
    "\n",
    "<a id=\"q132\"></a> \n",
    "\n",
    "Plot the training loss and accuracy per epoch (both train and validation) for your chosen architecture.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwXHX6i8x0MT",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic1uIduQx0Me"
   },
   "source": [
    "<a id=\"part2\"></a>\n",
    "    \n",
    "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
    "\n",
    "# PART 2: Regression with CNN \n",
    "<a id=\"part2intro\"></a>\n",
    "## Overview\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "**In this problem we will construct a neural network to predict how far a face is from being \"upright\"**. \n",
    "\n",
    "**Image orientation estimation**\n",
    "\n",
    "Image orientation estimation with convolutional networks was first implemented in 2015 by Fischer, Dosovitskiy, and Brox in a paper titled [\"Image Orientation Estimation with Convolutional Networks\"](https://lmb.informatik.uni-freiburg.de/Publications/2015/FDB15/image_orientation.pdf). In that paper, the authors trained a network to straighten a wide variety of images using the [Microsoft COCO dataset](https://cocodataset.org/#home). \n",
    "\n",
    "**The modified CelebA dataset**\n",
    "\n",
    "In order to have a reasonable training time for a homework, we will be working on a subset of the problem where we just straighten images of faces. To do this:\n",
    "\n",
    "- We will be using the [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset of celebrity faces, where we assume that professional photographers have taken level pictures;\n",
    "\n",
    "\n",
    "- The training will be supervised, with a rotated image (up to $\\pm 60^\\circ$) as an input, and the amount (in degrees) that the image has been rotated as a target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<a id=\"q21\"></a>\n",
    "\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise'>\n",
    "<b>2.1 Data preparation</b>\n",
    "<hr>\n",
    "<b>Q2.1.1</b>\n",
    "\n",
    "<a id=\"q211\"></a>\n",
    "\n",
    "**Loading the CelebA Dataset.** Run the cells provided to automatically load the CelebA dataset if you are on the cluster. Otherwise it will be downloaded for you. This is a TensorFlow Dataset object. [TensorFlow Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) are objects that behave like Python generators, allowing you to take elements (either input/target tuples or feature dictionaries) until you have gone through the entire dataset. Note how this is different from PART 1 where the entire dataset was loaded in as an array. Datasets also allow you to pipeline transformations to be applied to the elements, resulting in a new transformed Dataset (like `train_rot_ds`).\n",
    "\n",
    "  - **All you need to do is run the provided code**:\n",
    "\n",
    "    - The creation of the normalization/rotation/resize pipeline has been done for you, resulting in train dataset `train_rot_ds` and validation dataset `test_rot_ds`. \n",
    "    \n",
    "**Note:** You do not need to create a separate validation anywhere in Part 2. We are just using train and test with no validation for simplicity.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Define paths and URL\n",
    "cluster_data_dir = os.path.expanduser('~/142601/data/hw3')\n",
    "local_data_dir = './data'\n",
    "gdrive_url = 'https://drive.google.com/uc?id=13C6qxNmKlkW8ZRcFcc3N0GhmBeww8HDI'\n",
    "\n",
    "# Check for cluster data directory first\n",
    "if os.path.exists(cluster_data_dir):\n",
    "    print(f\"Using cluster dataset\")\n",
    "    data_dir = cluster_data_dir\n",
    "    download = False\n",
    "else:\n",
    "    # Not on cluster, prepare local directory\n",
    "    os.makedirs(local_data_dir, exist_ok=True)\n",
    "    \n",
    "    # If not already downloaded, get it from Google Drive\n",
    "    if not os.path.exists(os.path.join(local_data_dir, 'celeb_a')):\n",
    "        print(\"Downloading dataset...\")\n",
    "        targz_path = os.path.join(local_data_dir, 'celeb_a.tar.gz')\n",
    "        gdown.download(gdrive_url, targz_path)\n",
    "        \n",
    "        # Extract archive\n",
    "        with tarfile.open(targz_path) as tar:\n",
    "            tar.extractall(path=local_data_dir)\n",
    "        os.remove(targz_path)\n",
    "    \n",
    "    data_dir = local_data_dir\n",
    "    download = False\n",
    "\n",
    "# Load the dataset\n",
    "train_celeb, test_celeb = tfds.load(\n",
    "    \"celeb_a\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    shuffle_files=False,\n",
    "    data_dir=data_dir,\n",
    "    download=download,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def normalize_image(img):\n",
    "    \"\"\"Normalize image to [0, 1] range.\"\"\"\n",
    "    return tf.cast(img, tf.float32)/255.\n",
    "\n",
    "@tf.function\n",
    "def scipy_rotate(image, angle):\n",
    "    \"\"\"Efficient rotation using SciPy's ndimage.\"\"\"\n",
    "    rotated = tf.numpy_function(\n",
    "        lambda img, ang: ndimage.rotate(img, ang, reshape=False, order=1),\n",
    "        [image, angle],\n",
    "        tf.float32\n",
    "    )\n",
    "    rotated.set_shape(image.shape)\n",
    "    rotated = tf.image.resize_with_crop_or_pad(rotated, 140, 120)\n",
    "    return rotated\n",
    "\n",
    "@tf.function\n",
    "def tf_random_rotate_helper(image):\n",
    "    \"\"\"Apply random rotation.\"\"\"\n",
    "    image = normalize_image(image)\n",
    "    deg = tf.random.uniform([], -60.0, 60.0)\n",
    "    rotated = scipy_rotate(image, deg)\n",
    "    return rotated, deg\n",
    "\n",
    "@tf.function\n",
    "def tf_random_rotate_image(element):\n",
    "    \"\"\"Process dataset element.\"\"\"\n",
    "    image = element['image']\n",
    "    rotated, angle = tf_random_rotate_helper(image)\n",
    "    rotated.set_shape((140, 120, 3))\n",
    "    return rotated, angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VYXdRVY8x0Mm"
   },
   "outputs": [],
   "source": [
    "# Pipeline for creating randomly rotated images with their target labels being \n",
    "# the amount they were rotated, in degrees.\n",
    "train_rot_ds = train_celeb.map(\n",
    "    tf_random_rotate_image,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "test_rot_ds = test_celeb.map(\n",
    "    tf_random_rotate_image,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise'><b>Q2.1.2</b>\n",
    "\n",
    "<a id=\"q212\"></a>\n",
    "\n",
    "**Taking a look.** In a grid of subplots, plot at least 4 rotated images from `train_rot_ds` with the titles being the amount the images have been rotated. The floating point numbers in the titles should have a reasonable number of digits. **HINT:** one way to get a few image+label tuples from the Dataset is with `train_rot_ds.take(4)`. Check the [TensorFlow Datasets documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "VHy76uiox0Mo",
    "outputId": "243ef68a-9a00-4b25-bbc8-11cb81730118",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-V3cWPARx0Mf"
   },
   "source": [
    "<a id=\"q22\"></a>\n",
    "\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise'>\n",
    "<b>2.2 Building and training your CNN</b>\n",
    "<hr>\n",
    "<a id=\"q221\"></a>\n",
    "<b>Q2.2.1 Compiling your model.</b>\n",
    "    \n",
    "Construct a model with multiple Conv layers and any other layers you think would help. Be certain to print your model summary as always. Feel free to experiment with architectures and number of parameters if you wish to get better performance or better training speed. You certainly don't need more than a few million parameters; we have been able to do it with substantially fewer. \n",
    "    \n",
    "**Note:** Again, it is fine to attempt your own implementation of a well-known architecture, but you may not load any pre-constructed models. The network must be built layer-by-layer from your own code.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaD3HlKNx0Mr",
    "outputId": "17a0e1d3-29c7-43fb-adc2-a1dc365ed52f",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-V3cWPARx0Mf"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise'><b>Q2.2.2 Training your model</b>\n",
    "\n",
    "<a id=\"q222\"></a>\n",
    "\n",
    "Train your model. Please note that the `model.fit()` argument syntax is a little different when working with Datasets instead of numpy arrays; take a look at the [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) documentation. Be sure to also pass the test data as validation data. When passing `train_rot_ds` to `.fit()`, you will find it useful to use pipelines to [batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) the data. You can also experiment with [prefetching](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) batches/elements from the dataset, which may allow you to speed up iterations by a few percent. Finally, while dry-running and prototyping your model, you may find it useful to [take](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) a subset of the data to speed up experimentation. However, your final model MUST be trained on all the available training data! You should achieve test MSEs of less than 9, corresponding roughly to $\\pm 3^\\circ$ accuracy in predicting the rotations on the test set. This can be achieved in just 2-3 epochs, though you are free to train as long as you want.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_shHXiHx0Mt",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-V3cWPARx0Mf"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise'><b>Q2.2.3 Saving and loading your weights</b>\n",
    "\n",
    "<a id=\"q223\"></a>\n",
    "\n",
    "Save your model weights to the path `model/your_model_name` where `your_model_name` is whatever filename prefix you want. Then reload your weights from that same path.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFVM_NMbx0Mv",
    "outputId": "552aef1d-9a85-4dfa-e479-5611ffcc24fd",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-V3cWPARx0Mf"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise'><b>Q2.2.4 Evaluating your model</b>\n",
    "\n",
    "<a id=\"q224\"></a>\n",
    "\n",
    "Create a subplots grid with 4 rows and 3 columns. Each row will be a separate image from the test set (of your choice) and each column will consist of: Original Image, Predicted Straightened Image, Target Straightened Image. The title of the latter two should be the predicted rotation and the actual rotation. For example, a row should look something like the image shown below. This can be achieved using the provided function `rot_resize` to correct for the rotation predicted by your network.\n",
    "\n",
    "![straightened face](data/straightened.png)\n",
    "    \n",
    "**Hint:** Your network expects its input to have an explicit 'batch' dimension (the 1st dimension). If you use `take()` to get one image from your dataset, you will need to add this extra batch dimension to it in order to pass it to `predict()`. There are several ways to do this. You can check out [this SO post](https://stackoverflow.com/questions/43017017/keras-model-predict-for-a-single-image) for some ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def rot_resize(img, deg):\n",
    "    \"\"\"Rotate image by specified degrees and resize to 140x120.\"\"\"\n",
    "    # Ensure the input is a numpy array\n",
    "    if isinstance(img, tf.Tensor):\n",
    "        img = img.numpy()\n",
    "    \n",
    "    # Apply rotation using SciPy's ndimage\n",
    "    rotimg = ndimage.rotate(img, deg, reshape=False, order=3)\n",
    "    \n",
    "    # Clip values to valid range\n",
    "    rotimg = np.clip(rotimg, 0., 1.)\n",
    "    \n",
    "    # Resize using TensorFlow's crop_and_resize\n",
    "    rotimg = tf.convert_to_tensor(rotimg, dtype=tf.float32)\n",
    "    rotimg = tf.image.resize_with_crop_or_pad(rotimg, 140, 120)\n",
    "    \n",
    "    # Convert back to numpy for matplotlib\n",
    "    return rotimg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWsHliY1x0My",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise'>\n",
    "<a id=\"q23\"></a>\n",
    "<b>2.3 Further Analysis</b>\n",
    "<hr>\n",
    "<b>Q2.3.1 Correct an image of your choosing</b>\n",
    "\n",
    "<a id=\"q231\"></a>\n",
    "\n",
    "Find an image or image(s) (not from the provided test/training sets), or make your own; it does not necessarily have to be a human face. You may rotate it yourself up to $\\pm60^\\circ$, or the face can already be naturally rotated. Resize and crop the image to 140px by 120px, load it here, and normalize it to [0.,1.], and use your network to correct it.\n",
    "\n",
    "![Confused Chris](data/chrisprattcorrection.png)\n",
    "    \n",
    "**Note:** Please do *not* upload your custom image as a separate file with your notebook submission. It is sufficient to display your results in the cell output.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ptw3bIJcx0M0",
    "outputId": "c15561ac-4ba2-4cf8-f966-60ca7c127fee",
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic1uIduQx0Me"
   },
   "source": [
    "<a id=\"part3\"></a>\n",
    "    \n",
    "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
    "\n",
    "# PART 3: Image Segmentation\n",
    "<a id=\"part3intro\"></a>\n",
    "## Overview\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "In this section, we will use the Oxford-IIIT Pet dataset to design a model for an image segmentation task.\n",
    "\n",
    "[Semantic image segmentation](https://arxiv.org/abs/2302.06378) is a computer vision application where we assign a semantically meaningful label, or class, to every pixel in the image. For example, a camera on a self-driving car would segment the imagery around it into semantic labels such as \"Road\", \"Person\", \"Bike\", etc.\n",
    "\n",
    "Our  dataset contains images of various pet breeds along with their corresponding segmentation masks. Each raw image is 500x403 with three RGB channels, while the corresponding mask is of the same size but contains only a single channel, indicating the pixel class.\n",
    "\n",
    "For simplicity, we resize both the images and masks to 64x64 to speed up training.\n",
    "We have also have provided all the necessary preprocessing code for you as well as some functions for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with tensorflow-datasets\n",
    "ds_source = tfds.load(\n",
    "    'oxford_iiit_pet',\n",
    "    split='test', # train set has corrupted images :(\n",
    "    as_supervised=False, # we done need the dog breed labels\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "# Check the first example\n",
    "example = next(iter(ds_source))\n",
    "print(example.keys())\n",
    "print(f\"The raw image shape is: {example['image'].shape}\")\n",
    "print(f\"The raw segmentation mask shape is: {example['segmentation_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sample, reduce_class=False, resize_dim=64):\n",
    "    \"\"\"\n",
    "    Complete preprocessing function that:\n",
    "    1. Resizes image and mask to (resize_dim, resize_dim)\n",
    "    2. Normalizes image to [0,1] range\n",
    "    3. Remaps class indices to be more intuitive\n",
    "    4. Optionally reduces to 2 classes\n",
    "    5. Clips pixel values in case of corrupted jpegs\n",
    "    6. Returns (image, mask) tuple\n",
    "    \"\"\"\n",
    "    image = sample['image']\n",
    "    mask = sample['segmentation_mask']\n",
    "\n",
    "    # Resize using bilinear interpolation for image and nearest neighbor for mask\n",
    "    image = tf.image.resize(image, [resize_dim, resize_dim], method='bilinear')\n",
    "    mask = tf.image.resize(mask, [resize_dim, resize_dim], method='nearest')\n",
    "\n",
    "    # Normalize the image: convert from [0, 255] to [0, 1]\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    # Convert mask values from 1-indexed to 0-indexed\n",
    "    mask = tf.cast(mask, tf.int32) - 1\n",
    "\n",
    "    # Remap classes to be more intuitive\n",
    "    # Current: 0-pet border, 1-background, 2-pet interior\n",
    "    # Desired: 0-background, 1-border, 2-center\n",
    "    new_mask = tf.zeros_like(mask)\n",
    "    new_mask = tf.where(mask == 1, 0, new_mask)  # Map old background (1) to new background (0)\n",
    "    new_mask = tf.where(mask == 0, 1, new_mask)  # Map old border (0) to new border (1) \n",
    "    new_mask = tf.where(mask == 2, 2, new_mask)  # Keep center as class 2\n",
    "    \n",
    "    # Reduce problem difficulty if requested\n",
    "    if reduce_class:\n",
    "        # Combine border and center into a single \"pet\" class\n",
    "        # Now we have: 0-background, 1-pet (both border and center)\n",
    "        new_mask = tf.where(new_mask > 0, 1, 0)\n",
    "\n",
    "    # Handle corrupt JPEG data by ensuring valid values\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "\n",
    "    # Always return (image, mask) tuple\n",
    "    return image, new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for visualization\n",
    "def visualize_image_and_mask(dataset, num_examples=5):\n",
    "    \"\"\"Visualizes image and segmentation mask pairs from the dataset.\"\"\"\n",
    "    for i, (image, mask) in enumerate(dataset.take(num_examples)):\n",
    "        # Convert to numpy arrays\n",
    "        image = image.numpy()\n",
    "        mask = mask.numpy().squeeze()\n",
    "        \n",
    "        # Convert normalized image to uint8 for display\n",
    "        if image.dtype == np.float32 and image.max() <= 1.0:\n",
    "            display_image = (image * 255.0).astype('uint8')\n",
    "        else:\n",
    "            display_image = image.astype('uint8')\n",
    "        \n",
    "        mask = mask.astype(int)\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(8, 4))\n",
    "\n",
    "        # Plot the image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(display_image)\n",
    "        plt.title(\"Image\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Plot the segmentation mask\n",
    "        plt.subplot(1, 2, 2)\n",
    "        cmap_choice = 'gray' if mask.max() <= 1 else 'viridis'\n",
    "        plt.imshow(mask, cmap=cmap_choice, vmin=0, vmax=mask.max())\n",
    "        plt.title(\"Segmentation Mask\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "def visualize_predictions(dataset, model, num_examples=5, show_iou=False):\n",
    "    \"\"\"\n",
    "    Visualizes predictions from a segmentation model alongside ground truth.\n",
    "    \n",
    "    Args:\n",
    "        dataset: TensorFlow dataset containing images and masks\n",
    "        model: Trained segmentation model\n",
    "        num_examples: Number of examples to visualize\n",
    "        show_iou: Whether to compute and display IoU metrics (default: False)\n",
    "    \"\"\"\n",
    "    shuffled_dataset = dataset.shuffle(buffer_size=200)\n",
    "    \n",
    "    for i, (images, masks) in enumerate(shuffled_dataset.take(num_examples)):\n",
    "        # Check if the dataset is batched\n",
    "        if len(images.shape) == 4:  # Batched data\n",
    "            batch_size = images.shape[0]\n",
    "            random_index = random.randint(0, batch_size - 1)\n",
    "            image_for_pred = images[random_index]\n",
    "            mask = masks[random_index]\n",
    "        else:  # Unbatched data\n",
    "            image_for_pred = images\n",
    "            mask = masks\n",
    "            \n",
    "        # Convert to numpy and prepare for display\n",
    "        display_image = (image_for_pred * 255.0).numpy().astype('uint8')\n",
    "        \n",
    "        # Predict the segmentation mask\n",
    "        with tf.device('/CPU:0'):\n",
    "            pred_mask = model.predict(image_for_pred[tf.newaxis, ...], verbose=0)\n",
    "        pred_mask = tf.argmax(pred_mask, axis=-1)[0]\n",
    "        \n",
    "        # Calculate metrics for this example\n",
    "        true_mask = mask.numpy().squeeze()\n",
    "        pred_mask_np = pred_mask.numpy().squeeze()\n",
    "        \n",
    "        # Calculate pixel-wise accuracy\n",
    "        accuracy = (true_mask == pred_mask_np).mean()\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Plot the input image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(display_image)\n",
    "        plt.title(\"Input Image\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Plot the ground truth mask\n",
    "        plt.subplot(1, 3, 2)\n",
    "        cmap_choice = 'gray' if true_mask.max() <= 1 else 'viridis'\n",
    "        plt.imshow(true_mask, cmap=cmap_choice, vmin=0, vmax=true_mask.max())\n",
    "        plt.title(\"Ground Truth Mask\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Plot the predicted mask with metrics in the title\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred_mask_np, cmap=cmap_choice, vmin=0, vmax=true_mask.max())\n",
    "        \n",
    "        if show_iou:\n",
    "            # Calculate IoU for this example\n",
    "            intersection = np.logical_and(true_mask, pred_mask_np).sum()\n",
    "            union = np.logical_or(true_mask, pred_mask_np).sum()\n",
    "            iou = intersection / union if union > 0 else 1.0\n",
    "            plt.title(f\"Predicted Mask\\nAcc: {accuracy:.2%} | IoU: {iou:.2%}\")\n",
    "        else:\n",
    "            plt.title(f\"Predicted Mask\\nAcc: {accuracy:.2%}\")\n",
    "            \n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-V3cWPARx0Mf"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise'><b>Q3.1 Deep CNN for Image Segmentation</b>\n",
    "\n",
    "<a id=\"q31\"></a>\n",
    "\n",
    "Here you'll design a segmentation model that utilizes convolutional layers and skip connections. The model should perform a binary classification task, predicting the class for each pixel which is compared to the ground truth segmentation mask. \n",
    "\n",
    "Minimum Requirements:\n",
    "- A reasonably deep CNN architecture (you should be able to get above 80% validation accuracy with only 90 seconds of training)\n",
    "- Accepts an input size of 64 √ó 64 √ó 3 and performs pixel-level binary classification.\n",
    "- At least four skip connections placed appropriately.\n",
    "- Compile the model with an appropriate loss function and include accuracy as an evaluation metric.\n",
    "- Set the model name to `model_deepcnn`\n",
    "- Assign the result of `model_deepcnn.fit()` to a variable named `history_model_deepcnn`.\n",
    "\n",
    "We've created a train and validation set for you below.\n",
    "\n",
    "**HINT:** Typically, for binary classification we utilize the sigmoid activation function with a single output unit. That can be an appropriate method for this task as well, where we have a single output filter. However, considering a 2-class classification with a softmax activation function and two output units will also work. You may find a difference in performance between the two methods. Be sure you are selecting the appropriate loss for your choice.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create processed binary segmentation dataset\n",
    "ds_2c = ds_source.map(lambda sample: preprocess(sample, reduce_class=True),\n",
    "                               num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# 85/15 train-test split\n",
    "ds_2c_train, ds_2c_test = tf.keras.utils.split_dataset(ds_2c, left_size=0.85)\n",
    "# Reserve 20% of train as validation\n",
    "ds_2c_train, ds_2c_val = tf.keras.utils.split_dataset(ds_2c, left_size=0.80)\n",
    "\n",
    "# Dataset sizes\n",
    "print(\"Train:\", ds_2c_train.cardinality().numpy())\n",
    "print(\"Validation:\", ds_2c_val.cardinality().numpy())\n",
    "print(\"Test:\", ds_2c_test.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Here are some of the images and their corresponding segmentation masks. The segmentation mask categorizes each pixel into two classes: \"background\" or \"pet\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize 5 examples from the training split\n",
    "visualize_image_and_mask(ds_2c_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFVM_NMbx0Mv",
    "outputId": "552aef1d-9a85-4dfa-e479-5611ffcc24fd",
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define, compile, and train your model\n",
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Run the cell below to visualize your loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def plot_training_history(model_history_object, model_name=''):\n",
    "    # Extract the training history dictionary\n",
    "    hist_dict = model_history_object.history\n",
    "    # Plot\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    loss_color = 'coral'\n",
    "    # Plot Loss on the primary y-axis\n",
    "    loss_plot, = plt.plot(hist_dict['loss'],\n",
    "                          label='Training Loss', color=loss_color)\n",
    "    val_loss_plot, = plt.plot(hist_dict['val_loss'],\n",
    "                              label='Validation Loss', color=loss_color, linestyle='--')\n",
    "    \n",
    "    # Label for the primary y-axis (Loss)\n",
    "    plt.ylabel('Loss', color=loss_color, fontsize=18)\n",
    "    plt.gca().tick_params(axis='y', colors=loss_color)\n",
    "    ticks = range(0, len(hist_dict['loss'])+1)\n",
    "    plt.xticks(ticks=ticks, labels=[x+1 for x in ticks])\n",
    "    plt.xlabel('Epochs')  # X-axis label\n",
    "    plt.title(model_name)\n",
    "    \n",
    "    # Create a secondary y-axis for Accuracy\n",
    "    acc_color = 'lightseagreen'\n",
    "    ax2 = plt.gca().twinx()\n",
    "    train_acc = hist_dict.get('accuracy', hist_dict.get('acc'))\n",
    "    val_acc = hist_dict.get('val_accuracy', hist_dict.get('val_acc'))\n",
    "    accuracy_plot, = ax2.plot(train_acc,\n",
    "                              label='Training Accuracy', color=acc_color)\n",
    "    val_accuracy_plot, = ax2.plot(val_acc,\n",
    "                                  label='Validation Accuracy', color=acc_color, linestyle='--')\n",
    "    ax2.set_ylabel('Accuracy', color=acc_color, fontsize=18)\n",
    "    ax2.tick_params(axis='y', colors=acc_color)\n",
    "    \n",
    "    plt.grid()\n",
    "    lines = [loss_plot, val_loss_plot, accuracy_plot, val_accuracy_plot]\n",
    "    labels = [line.get_label() for line in lines]\n",
    "    plt.legend(lines, labels, loc='right', fontsize=8);\n",
    "\n",
    "plot_training_history(history_model_deepcnn, model_name=\"Deep CNN for Binary Image Segmenation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Run the cell below, and you will see your segmentation result. You can run the cell multiple times to see different result sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "visualize_predictions(ds_2c_test, model_deepcnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise'><b>[209] Q3.2 U-Net for 3-Class Image Segmentation</b>\n",
    "\n",
    "<a id=\"q32\"></a>  \n",
    "This question is required for 209 students but optional for others. Extra credit will not be given if attempted by non-209 students, but you may learn something new! :) \n",
    "\n",
    "In the previous model, the predictions were limited because we had to maintain the same dimensions throughout. This restricted the ability to capture different levels of detail. To address this, we introduce a new architecture: **U-Net**.  \n",
    "\n",
    "U-Net is a segmentation model originally designed for biomedical image segmentation. It follows an encoder-decoder structure with skip connections, allowing it to capture both fine details and high-level semantic features. This structure makes U-Net highly effective for pixel-wise classification. To learn more, please see:\n",
    "\n",
    "- [ED Slides and Notebook](https://edstem.org/us/courses/74185/lessons/133380/slides/750646)\n",
    "- **Reference Paper:** [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597)  \n",
    "\n",
    "You will build a U-Net model that takes a 64 √ó 64 √ó 3 image as input and classifies each pixel into one of three categories:  \n",
    "- Background  \n",
    "- Border-of-Pet\n",
    "- Pet  \n",
    "\n",
    "In the previous model, accuracy was used as a metric. However, for segmentation tasks, accuracy can be misleading. A model predicting all pixels as \"background\" could still appear accurate if there is a data imbalance. Instead, consider exploring alternative metrics. \n",
    "Refer to the links above for different approaches.  \n",
    "\n",
    "Minimum Requirements: \\\n",
    "To complete this task successfully, your model must:  \n",
    "- Follow an encoder-decoder architecture.  \n",
    "- Accept 64 √ó 64 √ó 3 input images and perform pixel-level 3-class classification.  \n",
    "- Include exactly three downsampling operations in the encoder, reducing the size as follows:  \n",
    "  - 64 -> 32 -> 16 -> 8  \n",
    "- Use a decoder that mirrors the encoder, upsampling back to 64 √ó 64.  \n",
    "- Implement at least three skip connections appropriately.  \n",
    "- Compile with an appropriate loss function and include accuracy as an evaluation metric.  \n",
    "- Name your model `model_unet`.  \n",
    "- Assign the result of `model_unet.fit()` to a named `history_model_unet`.  \n",
    "\n",
    "\n",
    "With a well-optimized model following the requirements above, you should be able above 80% validation accuracy within 4-5 minutes of training. If training takes significantly longer or the model performs poorly, you should inspect your the architecture and/or loss function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create processed 3-class segmentation dataset\n",
    "ds_3c = ds_source.map(lambda sample: preprocess(sample, reduce_class=False),\n",
    "                               num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# 85/15 train-test split\n",
    "ds_3c_train, ds_3c_test = tf.keras.utils.split_dataset(ds_3c, left_size=0.85)\n",
    "# Reserve 20% of train as validation\n",
    "ds_3c_train, ds_3c_val = tf.keras.utils.split_dataset(ds_3c_train, left_size=0.80)\n",
    "\n",
    "# Dataset sizes\n",
    "print(\"Train:\", ds_3c_train.cardinality().numpy())\n",
    "print(\"Validation:\", ds_3c_val.cardinality().numpy())\n",
    "print(\"Test:\", ds_3c_test.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Visualize 5 examples from the training split\n",
    "visualize_image_and_mask(ds_3c_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VFVM_NMbx0Mv",
    "outputId": "552aef1d-9a85-4dfa-e479-5611ffcc24fd",
    "scrolled": true,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Run the cell below to visualize your loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plot_training_history(history_model_unet,\n",
    "                      model_name=\"U-Net for 3-Class Image Segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Run the cell below, and you will see your segmentation result. You can run the cell multiple times to see different result sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_predictions(ds_3c_test, model_unet, show_iou=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise'><b>Wrap-up</b>\n",
    "\n",
    "* In a few sentences, please describe the aspect(s) of the assignment you found most challenging. This could be conceptual and/or related to coding and implementation.\n",
    "\n",
    "* How many hours did you spend working on this assignment? Store this as an int or float in `hours_spent_on_hw`. If you worked on the project in a group, report the *average* time spent per person.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "hours_spent_on_hw = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"wrapup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = time.time()\n",
    "print(f\"It took {(time_end - time_start)/60:.2f} minutes for this notebook to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This concludes HW3. Thank you!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ln75ejPkx0M1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "cs109b-hw4-solutions-colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "wrapup": {
     "name": "wrapup",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert float(hours_spent_on_hw), 'Please select a time in hours (int or float) to specify how long you spent on this assignment.'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
