{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7862a8f1",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Data Science 2: Advanced Topics in Data Science \n",
    "\n",
    "## Lab 02 (BONUS) - Bayesian Linear Regression\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Spring 2025**<br>\n",
    "**Instructors:** Pavlos Protopapas, Natesh Pillai, and Chris Gumb<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e27b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c273d42",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. **Motivating Bayesian Regression**\n",
    "\n",
    "1. **Bayesian Linear Regression: Known Variance**\n",
    "    - Decide on likelihood function\n",
    "    - Pick a prior\n",
    "    - Compute likelihood for given data\n",
    "    - Compute posterior\n",
    "    - Conjugate update rule\n",
    "    - Posterior predictive distribution\n",
    "        - Analytic\n",
    "        - Sampling\n",
    "\n",
    "1. **Bayesian Linear Regression: Unknown Variance/Precision**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "544e4362",
   "metadata": {},
   "source": [
    "## 1. Intro & Motivation\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Linear regression is a cornerstone of data science, offering a simple yet powerful way to model the linear relationship between a dependent variable and one or more independent variables. It's the go-to method for predictive modeling when the relationship between your variables is expected to be linear. In its simplest form, with one independent variable, the model predicts the dependent variable `Y` based on a linear combination of the independent variable `X` and an intercept term.\n",
    "\n",
    "The equation of a simple linear regression model is:\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1x + \\epsilon\n",
    "$$\n",
    "where $\\beta_0$ is the intercept, $\\beta_1$ is the slope coefficient, and $\\epsilon$ represents the error term.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e320c454",
   "metadata": {},
   "source": [
    "**Questions for Discussion**\n",
    "- What's wrong with our old (frequentist) Linear Regression?\n",
    "- But don't I already get uncertainty quantification with confidence intervals?\n",
    "- Why is MLE prone to overfitting?\n",
    "- But don't I have ways of addressing overfitting with the MLE approach?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29f93193",
   "metadata": {},
   "source": [
    "### Frequentist Paradigm\n",
    "\n",
    "In the frequentist paradigm, we often use Maximum Likelihood Estimation (**MLE**) to find the best-fitting parameters (`β`) that minimize the squared difference between the predicted and observed values. MLE provides **point estimates** for these parameters, aiming for the most likely values given the observed data.\n",
    "\n",
    "However, MLE does not directly offer a measure of uncertainty in these estimates beyond confidence intervals derived from assumptions about the error distribution. It also doesn't naturally incorporate prior knowledge about the parameters, which can be critical in cases of limited data or when we have strong reasons to believe parameters should have certain values based on previous research or theoretical considerations.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf053884",
   "metadata": {},
   "source": [
    "### Advantages of the Bayesian Approach\n",
    "\n",
    "The Bayesian approach to linear regression provides a probabilistic perspective, offering not just estimates of the regression coefficients but a full distribution of possible values. This distribution reflects our uncertainty about the coefficients, incorporating both the data (likelihood) and our prior beliefs (prior distribution).\n",
    "\n",
    "One key advantage of this method is its ability to update our beliefs about the model parameters in light of new data, a process known as Bayesian updating. Additionally, the Bayesian framework naturally incorporates prior knowledge, allowing for more informed and potentially more accurate estimates, especially in situations where data are scarce or noisy.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aee2df74",
   "metadata": {},
   "source": [
    "## 2. Bayesian Linear Regression (with Known Noise Variance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9547eaba",
   "metadata": {},
   "source": [
    "For our Bayesian linear regression model, we'll simplify the scenario by assuming that the noise variance (`σ²`) is known. This assumption allows us to focus on learning the regression coefficients using a **Normal-Normal conjugate model**. Recall that a 'conjugate' prior is a prior chosen such that the posterior distribution is the same type as the prior, simplifying calculations significantly.\n",
    "\n",
    "In our case, we'll assume Normal priors for our coefficients (`β`), which, combined with a Normal likelihood function (due to the assumption of normally distributed errors), ensures that our posterior distributions for the coefficients are also Normal!\n",
    "\n",
    "#### Bayesian Linear Regression with Multiple Coefficients\n",
    "\n",
    "In cases where our linear regression model includes multiple coefficients, such as an intercept $\\beta_0$ and slope $\\beta_1$, we extend our approach to accommodate the multivariate nature of our parameters. This involves adopting a multivariate normal distribution for both the prior and posterior distributions of the coefficients. Here, the mean of the distribution, $\\mu$, becomes a vector, and the variance is represented by a covariance matrix, $\\Sigma$, which captures the variances and covariances of the coefficients."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4618162",
   "metadata": {},
   "source": [
    "Here is some synthetic data we can attempt to model using Bayesian Linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca2d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Simulate data\n",
    "N = 100\n",
    "x = np.random.uniform(-1.5, 1.5, N) # Predictor\n",
    "b0 = -0.75  # True intercept\n",
    "b1 = 0.65  # True slope\n",
    "sigma2 = 1 # Known noise variance\n",
    "epsilon = np.random.normal(0, np.sqrt(sigma2), N).reshape(-1,1)\n",
    "\n",
    "# Generate exam scores\n",
    "y = b0 + b1 * x + epsilon\n",
    "\n",
    "# Alternatively, using the augmented design matrix X\n",
    "# and the vector of regression coefficients, beta\n",
    "X = np.vstack([np.ones_like(x), x]).T # add column of ones\n",
    "beta = np.array([b0, b1]).reshape(-1,1) # column vector of coeffs\n",
    "y = X@beta + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viz synthetic data\n",
    "plt.scatter(X[:,1], y, alpha=0.65, color='blue', label='Simulated Data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b605ff22",
   "metadata": {},
   "source": [
    "### Procedure Summary\n",
    "\n",
    "\n",
    "**Step 1: Likelihood function**\n",
    "\n",
    "An assumption of linear regression is that the response, $y$, is equal to some linear combination of predictors plus an intercept and a normally distributed noise term.\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_{p-1} x_{p-1} + \\beta_p x_p + \\epsilon$$\n",
    "Where $\\epsilon$ is the irriducible error, or 'noise' term, $\\epsilon \\sim N(0,\\sigma^2)$\n",
    "\n",
    "We can write this more compactly by adding a column of ones to our design matrix, $X$, and putting the intercept, $\\beta_0$, along with all the other regression coefficients in the column vector, $\\beta$.\n",
    "\n",
    "$$ \\vec{Y} = \\mathbf{X\\beta} + \\epsilon$$\n",
    "\n",
    "This is equivalent to the formulation:\n",
    "\n",
    "\n",
    "$$Y|x, \\beta \\sim N(x^T\\beta, \\sigma^2) $$\n",
    "\n",
    "And so\n",
    "\n",
    "$$E[Y|x,\\beta] = x^T\\beta$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68443ac6",
   "metadata": {},
   "source": [
    "**Step 2: Assume a prior distribution for the parameters**\n",
    "\n",
    "We need to choose a prior distribution for $\\mathbf{\\beta}$ before we get started, here we assume a multivariate normal distribution (conjugate!):\n",
    "\n",
    "$$\\mathbf{\\beta} \\sim N(\\vec{m}_0, \\Sigma_0)$$\n",
    "\n",
    "\n",
    "Initially, we can choose $\\vec{m}_0 = \\vec{\\mathbf{0}}$ and $\\Sigma_0 = \\alpha ^2I$\n",
    "\n",
    "(this ensures that the covariance matrix is diagonal with non-negative entries)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03b336b1",
   "metadata": {},
   "source": [
    "**Step 3: Compute the likelihood from the joint probability of the data, viewed as a function of the unknown parameters**\n",
    "\n",
    "Assume we have $n$ data pairs: $D = \\{(\\mathbf{x}^{(1)}, y^{(1)}), ..., (\\mathbf{x}^{(n)}, y^{(n)}) \\}$\n",
    "\n",
    "The likelihood of the parameter $\\mathbf{\\beta}$ is\n",
    "\n",
    "$$L(\\mathbf{\\beta} | D) = \\prod_{i=1}^n P(y^{(i)} | \\mathbf{x}^{(i)}, \\mathbf{\\beta})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7323a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential exercise: comparing the PDF to the likelihood function\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "840a3c5b",
   "metadata": {},
   "source": [
    "**Step 4: Obtain the posterior density via Bayes rule**\n",
    "\n",
    "#### The Multivariate Bayesian Update\n",
    "\n",
    "For multiple regression coefficients, the prior distribution of $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\dots, \\beta_n]^T$ is a multivariate normal distribution, expressed as $N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, where:\n",
    "\n",
    "- $\\boldsymbol{\\mu}$ is the vector of prior means for the coefficients.\n",
    "- $\\boldsymbol{\\Sigma}$ is the covariance matrix representing the variances and covariances of the coefficients.\n",
    "\n",
    "Given a design matrix $X$ and a vector of outcomes $Y$, the likelihood of observing $Y$ given $X$ and $\\boldsymbol{\\beta}$ is normally distributed. The goal is to update our beliefs about $\\boldsymbol{\\beta}$ given the observed data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c3f0ba3",
   "metadata": {},
   "source": [
    "**Step 5: Analyze the posterior distribution if possible**\n",
    "\n",
    "Often you'd like to summarize the posterior by looking at the expected value, variance, etc.\n",
    "\n",
    "This is easy when using a conjugate prior as the posterior belongs to the same family as the prior and we have closed form expressions for these values for named distribution families."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19419ffc",
   "metadata": {},
   "source": [
    "**Step 6: Deriving the Posterior Distribution**\n",
    "\n",
    "The posterior distribution of $\\boldsymbol{\\beta}$, given the data $Y$, is also a multivariate normal distribution $N(\\boldsymbol{\\mu_{post}}, \\boldsymbol{\\Sigma_{post}})$. The parameters of this posterior distribution can be updated using the following formulas:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma_{post}} = \\left(\\boldsymbol{\\Sigma}^{-1} + \\frac{X^TX}{\\sigma^2}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu_{post}} = \\boldsymbol{\\Sigma_{post}} \\left(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu} + \\frac{X^TY}{\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma^2$ is the known variance of the noise in the outcome $Y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d50c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior mean vector and covariance matrix for the coefficients\n",
    "p = X.shape[1]-1\n",
    "mu_prior = np.zeros(p+1).reshape(-1,1)  # Adjust based on your prior knowledge\n",
    "a = 3\n",
    "Sigma_prior = a**2*np.eye(p+1)  # Identity matrix for simplicity, adjust as needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db29a6a1",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "Use the update rules above to complete the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab3af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_posterior(X, y, sigma2, mu_prior, Sigma_prior):\n",
    "    \"\"\"\n",
    "    Correctly updates the posterior mean and covariance matrix for a Bayesian linear regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Design matrix (or vector) of predictors. Should be 2D (n, p) for full data or 1D (p,) for a single observation.\n",
    "    - y: Vector of outcomes. Should be 1D (n,) for full data or a scalar for a single observation.\n",
    "    - sigma2: Known variance of the noise in the outcomes.\n",
    "    - mu_prior: Prior (or most recent posterior) mean vector of the regression coefficients.\n",
    "    - Sigma_prior: Prior (or most recent posterior) covariance matrix of the regression coefficients.\n",
    "\n",
    "    Returns:\n",
    "    - mu_post: Updated posterior mean vector of the regression coefficients.\n",
    "    - Sigma_post: Updated posterior covariance matrix of the regression coefficients.\n",
    "    \"\"\"\n",
    "    # Reshape X and Y for single observations\n",
    "    X = np.atleast_2d(X)  # Ensures X is 2D even for a single observation\n",
    "    y = np.atleast_1d(y)  # Ensures Y is 1D even for a scalar\n",
    "\n",
    "    # Calculate updates... mind the shape of arrays!!\n",
    "    Sigma_post = ...\n",
    "    mu_prior_reshaped = ...\n",
    "\n",
    "    return mu_post, Sigma_post"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5aec01f",
   "metadata": {},
   "source": [
    "These are our posterior parameters after observing all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_post, Sigma_post = update_posterior(X, y, sigma2, mu_prior, Sigma_prior)\n",
    "\n",
    "print(\"Posterior Mean:\", mu_post)\n",
    "print(\"Posterior Covariance Matrix:\\n\", Sigma_post)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44fee2a6",
   "metadata": {},
   "source": [
    "This function will compute a sequence of posterior parameters derived from viewing chunks of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_posterior_distributions(X, y, sigma2, mu_prior, Sigma_prior, update_every=5):\n",
    "    \"\"\"\n",
    "    Accumulate parameters of posterior distributions after observing every 'update_every' data points.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Design matrix with observations as rows and features as columns.\n",
    "    - y: Vector of outcomes corresponding to each row in X.\n",
    "    - sigma2: Known variance of the noise in the outcomes.\n",
    "    - mu_prior: Initial prior mean vector for the regression coefficients.\n",
    "    - Sigma_prior: Initial prior covariance matrix for the regression coefficients.\n",
    "    - update_every: Number of new observations after which to update the posterior.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of tuples, each containing the mean vector and covariance matrix of the original prior\n",
    "      followed by those of the new posterior distribution at each update interval.\n",
    "    \"\"\"\n",
    "    n_points = len(y)\n",
    "    posterior_params = [(mu_prior.flatten(), Sigma_prior)]\n",
    "    \n",
    "    for i in range(0, n_points, update_every):\n",
    "        X_current = X[:i + update_every]\n",
    "        y_current = y[:i + update_every]\n",
    "        mu_post, Sigma_post = update_posterior(X_current, y_current, sigma2, mu_prior, Sigma_prior)\n",
    "        \n",
    "        # Store the parameters of the current posterior distribution\n",
    "        posterior_params.append((mu_post.flatten(), Sigma_post))\n",
    "        \n",
    "        # Update mu_prior and Sigma_prior for the next iteration\n",
    "        mu_prior, Sigma_prior = mu_post, Sigma_post\n",
    "    \n",
    "    return posterior_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f97ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_params = accumulate_posterior_distributions(X, y, sigma2, mu_prior, Sigma_prior, update_every=1)\n",
    "\n",
    "mu_post = posterior_params[-1][0]\n",
    "Sigma_post = posterior_params[-1][1]\n",
    "print(\"Posterior Mean:\", mu_post)\n",
    "print(\"Posterior Covariance Matrix:\\n\", Sigma_post)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98223c42",
   "metadata": {},
   "source": [
    "### Visualize Evolution of Regression Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021261ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_updates(X, y, sigma2, mu_prior, Sigma_prior, update_every=5):\n",
    "    \"\"\"\n",
    "    Plot regression line updates after observing every 'update_every' data points, incorporating the latest\n",
    "    `update_posterior` implementation. Each subplot shows the regression line based on the data observed so far,\n",
    "    with new data points highlighted.\n",
    "    \n",
    "    Parameters are as described in the original function.\n",
    "    \"\"\"\n",
    "    n_points = len(y)\n",
    "    n_updates = n_points // update_every\n",
    "\n",
    "    # Calculate grid size for subplots to not exceed 5 columns\n",
    "    n_rows = int(np.ceil(n_updates / 5))\n",
    "    n_cols = min(n_updates, 5)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(n_updates):\n",
    "        X_current = X[:(i+1)*update_every]\n",
    "        y_current = y[:(i+1)*update_every]\n",
    "        mu_post, Sigma_post = update_posterior(X_current, y_current, sigma2, mu_prior, Sigma_prior)\n",
    "\n",
    "    # Plot all data points observed so far\n",
    "        axes[i].scatter(X_current[:, 1], y_current, color='blue', label='Previous Points')\n",
    "        if i < n_updates - 1:\n",
    "            # Highlight new points in a different color\n",
    "            axes[i].scatter(X[(i*update_every):(i+1)*update_every, 1], y[(i*update_every):(i+1)*update_every], color='red', label='New Points')\n",
    "\n",
    "        # Calculate and plot the regression line\n",
    "        x_line = np.linspace(X[:,1].min(), X[:,1].max(), 100)\n",
    "        X_line = np.vstack([np.ones_like(x_line), x_line]).T # add column of ones\n",
    "        y_line = X_line@mu_post\n",
    "        axes[i].plot(x_line, y_line, color='black', label='Regression Line')\n",
    "\n",
    "        axes[i].legend()\n",
    "        axes[i].set_xlabel('X')\n",
    "        axes[i].set_title(f'{(i+1)*update_every} Data Points')\n",
    "\n",
    "    axes[0].set_ylabel('y')\n",
    "\n",
    "    # Ensure axes not used are hidden, for cases where n_updates < n_rows * n_cols\n",
    "    for j in range(n_updates, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980110fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_updates(X, y, sigma2, mu_prior, Sigma_prior, update_every=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e986294",
   "metadata": {},
   "source": [
    "## Visualizing of Evolution of Posterior\n",
    "\n",
    "Visualizing the posterior distributions of the coefficients can provide deeper insights into the relationships between predictors and the response variable. For two coefficients, consider plotting contour lines representing the joint posterior distribution, which can illustrate not only the individual uncertainties but also how the coefficients might co-vary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2728a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior(coef_dist):\n",
    "    fig, axs = plt.subplots(4, 5, figsize=(27, 20))\n",
    "    x, y = np.mgrid[-1.5:1:.01, -1:1.5:.01]\n",
    "    pos = np.dstack((x, y))\n",
    "    fig.suptitle(\"Updating of Posterior Distribution\")\n",
    "    for i in range(min([20, len(coef_dist)])):\n",
    "        rv = coef_dist[i]\n",
    "        im = axs[i//5][i%5].contourf(x, y, rv.pdf(pos))\n",
    "        axs[i//5][i%5].set_title(f\"{i*5} data points\")\n",
    "        axs[i//5][i%5].set_xlabel(\"$\\\\beta_0$\")\n",
    "        axs[i//5][i%5].set_ylabel(\"$\\\\beta_1$\")\n",
    "    cbar_ax = fig.add_axes([0.95, 0.15, 0.01, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax)\n",
    "    lines = axs[-1][-1].get_legend_handles_labels()[0]\n",
    "    labels = axs[-1][-1].get_legend_handles_labels()[1]\n",
    "    # fig.legend(lines, labels, loc='upper right')\n",
    "\n",
    "coef_dists = [multivariate_normal(mu, cov) for (mu, cov) in posterior_params]\n",
    "plot_posterior(coef_dists)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47c17ab6",
   "metadata": {},
   "source": [
    "## Understanding the Posterior Predictive Distribution\n",
    "\n",
    "The posterior predictive distribution represents our predictions for new, unseen data points, incorporating all the knowledge we've accumulated from the observed data. It is derived by integrating the joint probability of the new data point and the parameters over the entire posterior distribution of the parameters. This process effectively averages predictions over all possible parameter values, weighted by their posterior probability.\n",
    "\n",
    "### The Posterior Predictive Distribution as a Marginal Distribution\n",
    "\n",
    "Given:\n",
    "- Observed data $Y$ and predictors $X$.\n",
    "- New predictors $X_{\\text{new}}$ for which we want to predict the corresponding $Y_{\\text{new}}$.\n",
    "\n",
    "The posterior predictive distribution for $Y_{\\text{new}}$ given $X_{\\text{new}}$ and observed data $(X, Y)$ is calculated by marginalizing over the posterior distribution of the parameters $\\beta$:\n",
    "\n",
    "$$\n",
    "p(Y_{\\text{new}} | X_{\\text{new}}, X, Y) = \\int p(Y_{\\text{new}} | X_{\\text{new}}, \\beta) p(\\beta | X, Y) d\\beta\n",
    "$$\n",
    "\n",
    "- $p(Y_{\\text{new}} | X_{\\text{new}}, \\beta)$ is the likelihood of observing $Y_{\\text{new}}$ given $X_{\\text{new}}$ and parameters $\\beta$.\n",
    "- $p(\\beta | X, Y)$ is the posterior distribution of $\\beta$ given the observed data.\n",
    "\n",
    "### Analytic Derivation for Normal-Normal Model\n",
    "\n",
    "In the context of Bayesian linear regression with a normal likelihood and a normal prior on $\\beta$ (the normal-normal model):\n",
    "\n",
    "- Likelihood: $Y | X, \\beta \\sim \\mathcal{N}(X^T\\beta, \\sigma^2)$\n",
    "- Prior: $\\beta \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)$\n",
    "- Posterior of $\\beta$: Given the conjugacy, $\\beta | X, Y \\sim \\mathcal{N}(\\mu_{\\text{post}}, \\Sigma_{\\text{post}})$\n",
    "\n",
    "The posterior predictive distribution for $Y_{\\text{new}}$ is then also normally distributed, with mean and variance derived from the formula above:\n",
    "\n",
    "$$\n",
    "Y_{\\text{new}} | X_{\\text{new}}, X, Y \\sim \\mathcal{N}(X_{\\text{new}}^T \\mu_{\\text{post}}, \\sigma^2 + X_{\\text{new}}^T \\Sigma_{\\text{post}} X_{\\text{new}})\n",
    "$$\n",
    "\n",
    "### Intuition and Summary\n",
    "\n",
    "The posterior predictive distribution integrates out the uncertainty in the parameters $\\beta$, providing a predictive distribution that accounts for both the aleatoric uncertainty (inherent data noise) and epistemic uncertainty (parameter uncertainty). This results in a comprehensive predictive distribution that reflects all sources of uncertainty in our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming mu_post, Sigma_post, and sigma2 are defined from your Bayesian linear regression model\n",
    "# mu_post: Posterior mean vector of the regression coefficients\n",
    "# Sigma_post: Posterior covariance matrix of the regression coefficients\n",
    "# sigma2: Known variance of the noise in the outcomes\n",
    "\n",
    "# Example new predictors X_new\n",
    "x_line = np.linspace(X[:,1].min(), X[:,1].max(), 100)\n",
    "X_new = np.vstack([np.ones_like(x_line), x_line]).T # add column of ones and squared term\n",
    "\n",
    "# Calculate mean and variance of the posterior predictive distribution for each new data point\n",
    "mu_post = posterior_params[-1][0]\n",
    "Sigma_post = posterior_params[-1][1]\n",
    "post_pred_mean = X_new @ mu_post  # Mean of the posterior predictive distribution\n",
    "\n",
    "# For the variance, we need to calculate X_new^T Sigma_post X_new for each data point, plus sigma2\n",
    "post_pred_var = np.array([x_new @ Sigma_post @ x_new.T for x_new in X_new]) + sigma2\n",
    "\n",
    "# Generate predictions from the posterior predictive distribution\n",
    "# Here we draw one sample for each new data point to illustrate, you could draw more samples if needed\n",
    "predictions = np.random.normal(post_pred_mean, np.sqrt(post_pred_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d63c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], y, label='original data')\n",
    "plt.scatter(X_new[:,1], predictions, label='simulated data (analytic)')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1247fa31",
   "metadata": {},
   "source": [
    "## Multi-Step Monte Carlo Simulation for Posterior Predictive Sampling\n",
    "\n",
    "As an alternative to the analytic posterior predictive distribution, we can use a Monte Carlo simulation to generate samples. This approach is especially useful in complex models or when an analytic solution is not straightforward.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Sample $\\beta$**: Draw $M$ samples $\\{\\beta^{(m)}\\}_{m=1}^M$ from the posterior distribution of $\\beta$, $\\beta | Y \\sim \\mathcal{N}(\\mu_{\\text{post}}, \\Sigma_{\\text{post}})$.\n",
    "\n",
    "2. **Generate Predictive Samples**: For each sampled $\\beta^{(m)}$, generate a predictive sample for a new observation $Y_{\\text{new}}^{(m)}$ using $X_{\\text{new}}$:\n",
    "    - Compute the mean $X_{\\text{new}}^T \\beta^{(m)}$.\n",
    "    - Draw $Y_{\\text{new}}^{(m)}$ from $\\mathcal{N}(X_{\\text{new}}^T \\beta^{(m)}, \\sigma^2)$.\n",
    "\n",
    "3. **Analysis**: The collection $\\{Y_{\\text{new}}^{(m)}\\}_{m=1}^M$ represents samples from the posterior predictive distribution. Analyze these samples to estimate predictive means, intervals, or other quantities of interest.\n",
    "\n",
    "### Implementation\n",
    "This Monte Carlo approach can be implemented using numerical libraries such as NumPy for sampling and matrix operations, and SciPy for working with distributions. It offers a practical alternative to the analytic derivation, providing a computational method to explore the posterior predictive distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e0a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Number of samples to draw from the posterior distribution of beta\n",
    "n_samples = 1000\n",
    "\n",
    "# Sample a beta vector from its posterior distribution for each x_new\n",
    "beta_samples = np.random.multivariate_normal(mu_post, Sigma_post, n_samples)\n",
    "\n",
    "# Initialize an array to store predictive sample for each X_new\n",
    "predictive_samples = np.zeros([X_new.shape[0], n_samples])\n",
    "\n",
    "# For each sampled beta, generate predictive samples\n",
    "for i, beta in enumerate(beta_samples):\n",
    "    # Compute predictive mean for new data points\n",
    "    predictive_mean = X_new @ beta\n",
    "    # Sample from the normal distribution with the computed mean and known variance sigma2\n",
    "    predictive_samples[:, i] = np.random.normal(predictive_mean, np.sqrt(sigma2))\n",
    "\n",
    "# Calculate mean and percentile intervals (e.g., 95% CI) for the predictive samples\n",
    "predictive_mean = np.mean(predictive_samples, axis=1)\n",
    "lower_percentile = np.percentile(predictive_samples, 2.5, axis=1)\n",
    "upper_percentile = np.percentile(predictive_samples, 97.5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29290046",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_samples = np.random.multivariate_normal(mu_post, Sigma_post, X_new.shape[0])\n",
    "predictive_samples = np.zeros((X_new.shape[0]))\n",
    "for i, beta in enumerate(beta_samples):\n",
    "    # Compute predictive mean for new data points\n",
    "    predictive_mean = X_new[i] @ beta\n",
    "    # Sample from the normal distribution with the computed mean and known variance sigma2\n",
    "    predictive_samples[i] = np.random.normal(predictive_mean, np.sqrt(sigma2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Number of samples to draw from the posterior distribution of beta\n",
    "n_samples = 1000\n",
    "\n",
    "# Sample beta from its posterior distribution\n",
    "beta_samples = np.random.multivariate_normal(mu_post, Sigma_post, n_samples)\n",
    "\n",
    "# Initialize an array to store predictive samples for each X_new\n",
    "predictive_samples = np.zeros((X_new.shape[0], n_samples))\n",
    "\n",
    "# For each sampled beta, generate predictive samples\n",
    "for i, beta in enumerate(beta_samples):\n",
    "    # Compute predictive mean for new data points\n",
    "    predictive_mean = X_new @ beta\n",
    "    # Sample from the normal distribution with the computed mean and known variance sigma2\n",
    "    predictive_samples[:, i] = np.random.normal(predictive_mean, np.sqrt(sigma2))\n",
    "\n",
    "# Calculate mean and percentile intervals (e.g., 95% CI) for the predictive samples\n",
    "predictive_mean = np.mean(predictive_samples, axis=1)\n",
    "lower_percentile = np.percentile(predictive_samples, 2.5, axis=1)\n",
    "upper_percentile = np.percentile(predictive_samples, 97.5, axis=1)\n",
    "\n",
    "# Example plotting the mean prediction and confidence intervals\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_line, predictive_mean, label='Predictive Mean')\n",
    "plt.fill_between(x_line, lower_percentile, upper_percentile, color='gray', alpha=0.5, label='95% Predictive Interval')\n",
    "plt.scatter(X[:, 1], y, color='red', s=10, label='Observed Data')  # Assuming Y are the observed responses\n",
    "plt.xlabel('Predictor')\n",
    "plt.ylabel('Response')\n",
    "plt.legend()\n",
    "plt.title('Posterior Predictive Distribution with 95% Predictive Interval')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a4ebc9c",
   "metadata": {},
   "source": [
    "## 3. Bayesian Linear Regression with Unknown Noise Variance\n",
    "\n",
    "In the Bayesian framework, when both the regression coefficients and the noise variance are unknown, a common approach is to use a Normal-Gamma conjugate prior. This model assumes:\n",
    "\n",
    "- The likelihood of observing data $Y$ given predictors $X$ and parameters $\\beta$ and $\\sigma^2$ is $Y | X, \\beta, \\sigma^2 \\sim \\mathcal{N}(X\\beta, \\sigma^2I)$.\n",
    "- The prior over $\\beta$ is a multivariate normal distribution, $\\beta \\sim \\mathcal{N}(\\mu_0, \\sigma^2 \\Lambda_0^{-1})$, where $\\mu_0$ is the prior mean and $\\Lambda_0$ is the precision matrix (inverse of the covariance matrix) scaled by $\\sigma^2$.\n",
    "- The prior over the precision $\\tau$ (inverse of the variance $\\sigma^2$) follows a Gamma distribution, $\\tau \\sim \\text{Gamma}(\\alpha_0, \\beta_0)$, where $\\alpha_0$ and $\\beta_0$ are the shape and rate parameters of the Gamma distribution, respectively.\n",
    "\n",
    "The Normal-Gamma conjugate prior allows for an analytical solution to the posterior distributions of $\\beta$ and $\\tau$, incorporating all information from the data and the prior beliefs about the parameters.\n",
    "\n",
    "### Deriving the Posterior Distributions\n",
    "\n",
    "Given new data $(X, Y)$, the posterior distributions for $\\beta$ and $\\tau$ can be analytically derived, leading to:\n",
    "\n",
    "- The posterior distribution of $\\beta$ given $\\tau$ and data is a multivariate normal distribution with updated mean $\\mu_{\\text{post}}$ and precision matrix $\\Lambda_{\\text{post}}$.\n",
    "- The posterior distribution of $\\tau$ is a Gamma distribution with updated shape $\\alpha_{\\text{post}}$ and rate $\\beta_{\\text{post}}$.\n",
    "\n",
    "These posterior distributions incorporate the evidence from the data, updating our beliefs about the regression coefficients and the noise variance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef492754",
   "metadata": {},
   "source": [
    "### Prior Specification\n",
    "\n",
    "In this Bayesian linear regression model with unknown precision, we specify our priors as follows to ensure conjugacy:\n",
    "\n",
    "- **Coefficients Prior**: For the regression coefficients β, given the precision τ, we use a multivariate normal distribution: $$\\beta | \\tau \\sim \\mathcal{N}(\\mu_0, (\\tau \\Lambda_0)^{-1})$$ where $\\mu_0$ is the prior mean vector and $\\Lambda_0$ is the prior precision matrix for β.\n",
    "\n",
    "- **Precision Prior**: For the precision of the noise, we use a Gamma distribution: $$\\tau \\sim \\Gamma(\\alpha_0, \\beta_0)$$ where $\\alpha_0$ is the shape parameter and $\\beta_0$ is the rate parameter of the Gamma distribution.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9122dbd3",
   "metadata": {},
   "source": [
    "### Posterior Update Rules for Bayesian Linear Regression with Unknown Precision\n",
    "\n",
    "After observing the data \\(Y\\), we update our beliefs about the regression coefficients (\\(\\beta\\)) and the precision (\\(\\tau\\)), employing Bayesian inference. The update rules for the posterior distributions, given our Normal-Gamma conjugate prior setup, are as follows:\n",
    "\n",
    "1. **Posterior Distribution of Regression Coefficients ($\\beta$)**:\n",
    "   The posterior distribution of \\(\\beta\\), given \\(\\tau\\) and the data \\(Y\\), follows a Multivariate Normal distribution. The update rules for the parameters of this distribution are derived from the conjugate prior relationship, leading to:\n",
    "   - **Posterior Mean ($\\beta_N$)**: \n",
    "     $$ \\beta_N = \\Lambda_N^{-1} (X^TY + \\Lambda_0 \\beta_0) $$\n",
    "   - **Precision Matrix ($\\Lambda_N$)**: \n",
    "     $$ \\Lambda_N = \\Lambda_0 + X^TX $$\n",
    "\n",
    "2. **Posterior Distribution of Precision ($\\tau$)**:\n",
    "   The posterior distribution of \\(\\tau\\), after observing the data \\(Y\\), follows a Gamma distribution. The update rules for the Gamma distribution parameters are:\n",
    "   - **Shape Parameter ($a_N$)**: \n",
    "     $$ a_N = a + \\frac{N}{2} $$\n",
    "   - **Rate Parameter ($b_N$)**: \n",
    "     $$ b_N = b + \\frac{1}{2} \\left( Y^TY + \\beta_0^T\\Lambda_0\\beta_0 - \\beta_N^T\\Lambda_N\\beta_N \\right) $$\n",
    "\n",
    "These update rules allow us to analytically determine the posterior distributions of \\(\\beta\\) and \\(\\tau\\), reflecting our updated beliefs based on the observed data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8433f17",
   "metadata": {},
   "source": [
    "### Analytic Posterior Predictive Distribution\n",
    "\n",
    "The posterior predictive distribution for a new observation $x_{new}$ can be analytically derived under the Gaussian noise model. Given the conjugate Normal-Gamma prior and the linear model assumptions, the predictive distribution for a new observation $y_{new}$, given new data $x_{new}$, is:\n",
    "\n",
    "$$y_{new} | x_{new}, X, y \\sim t_{2\\alpha_n} \\left( x_{new}^T \\mu_n, \\frac{\\beta_n}{\\alpha_n} \\left(1 + x_{new}^T (\\Lambda_n)^{-1} x_{new} \\right) \\right)$$\n",
    "\n",
    "This distribution is a non-standardized Student's t-distribution, indicating the predictive uncertainty in $y_{new}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "456db1e9",
   "metadata": {},
   "source": [
    "### Sampling Approach to Posterior Predictive\n",
    "\n",
    "Though we have an analytic form for the posterior predictive distribution, sampling provides an alternative approach that is often easier to implement and interpret, especially in more complex models or for extensive posterior analysis. Here, we demonstrate how to perform this sampling using NumPy.\n",
    "\n",
    "1. Draw samples of τ from its posterior Gamma distribution.\n",
    "2. Given each sampled τ, draw β from its conditional posterior multivariate normal distribution.\n",
    "3. Use the sampled β and τ to draw samples from the predictive distribution for new observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5338490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior parameters\n",
    "a = 2.0  # Shape parameter for Gamma prior on τ\n",
    "b = 2.0  # Rate parameter for Gamma prior on τ\n",
    "Lambda_0 = np.eye(2)  # Prior precision matrix for β\n",
    "beta_0 = np.zeros(2)  # Prior mean for β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c8ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update rules to calculate posterior parameters\n",
    "y = y.flatten()\n",
    "Lambda_N = Lambda_0 + X.T @ X\n",
    "beta_N = np.linalg.inv(Lambda_N) @ (X.T @ y + Lambda_0 @ beta_0)\n",
    "a_N = a + N / 2\n",
    "b_N = b + 0.5 * (y.T@y + beta_0.T @ Lambda_0 @ beta_0 - beta_N.T @ Lambda_N @ beta_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a32dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43402524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from the posterior distributions\n",
    "n_samples = 1000\n",
    "tau_samples = np.random.gamma(a_N, 1 / b_N, n_samples)  # Sampling τ from its posterior Gamma distribution\n",
    "beta_samples = [np.random.multivariate_normal(beta_N, np.linalg.inv(tau * Lambda_N)) for tau in tau_samples]\n",
    "\n",
    "# To store samples from the posterior predictive distribution for new observations X_new\n",
    "predictive_samples = np.zeros((X_new.shape[0], n_samples))\n",
    "\n",
    "# For each sampled tau and beta, generate predictive samples\n",
    "for i, (tau, beta) in enumerate(zip(tau_samples, beta_samples)):\n",
    "    # Compute predictive mean for new data points\n",
    "    predictive_mean = X_new @ beta\n",
    "    # Sample from the normal distribution with the computed mean and precision\n",
    "    predictive_samples[:, i] = np.random.normal(X_new @ beta, 1 / np.sqrt(tau))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f751776",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], y, label='original data')\n",
    "plt.scatter(X_new[:,1], predictive_samples[:,1], label='simulated data (simulated)')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3917d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
